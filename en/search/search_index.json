{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>This repository serves as a practical tutorial for using <code>dlt</code> (dlthub.com). It collects step\u2011by\u2011step lessons, example code, and configuration notes designed to help readers quickly understand the core concepts and reproduce the examples locally.</p> <p>Follow the Getting Started section to set up the environment, then progress through the numbered tutorials and examples in the <code>dlt_tutorial/</code> directory.</p>"},{"location":"0-introduction/","title":"Introduction","text":"<p>This is the documentation for the <code>dlt</code> (data loading toolkit) tutorial, being. Do not confuse <code>dlt</code> from <code>Delta Live Tables</code> by Databricks. This has nothing to do with it.</p> <p>Use <code>\"dlthub\"</code> as a keyword to search for related content in search engines</p> <p>Otherwise you may find unrelated content about Delta Live Tables. When we say <code>dlt</code> in this documentation, we always refer to <code>data loading toolkit</code>.</p> <p>In data pipelines, a common acronym used is ETL (Extract, Transform, Load).</p> <p>Extracting has to do with getting data from a source system (e.g., an API, a database, files, etc). Transforming has to do with cleaning, normalizing, and shaping the data to fit the target system. Loading has to do with writing the data into the target system (e.g., a data warehouse, a data lake, etc).</p> <p>Alternatively, a more recent paradigm is ELT (Extract, Load, Transform). This reverses the order of the last two steps, loading the raw data into the target system first, and then transforming it there. This allows for more flexibility and scalability, loading data first and transforming it later as needed. This can be especially useful when dealing with large volumes of data or when the transformation logic is complex and may change over time.</p> <p><code>dlt</code> focuses on the loading part (<code>L</code>) of these paradigms. Our main requirement is to move data from a source system to a target system, while granting some flexibility on when and how to transform the data. The code for the loading part is mostly repetitive, for some common scenarios.</p> <p>Most of the time, data loading is not a one-time task. Data is continuously generated in source systems, and we need to keep our target systems up-to-date with the latest data. This is where <code>dlt</code> shines, providing features for incremental loading, change data capture, and scheduling.</p>"},{"location":"0-introduction/#alternatives-to-dlt","title":"Alternatives to <code>dlt</code>","text":"<p>There are many alternatives to <code>dlt</code> that address the data loading tasks. The list is huge and I'm not listing everything,, but here are the ones that I have used and/or consider relevant:</p> <ul> <li>Apache NiFi: An open-source data integration tool that supports data routing, transformation, and system mediation logic. It provides a web-based interface for designing data flows and supports a wide range of data sources and destinations. See https://nifi.apache.org/</li> <li>Airbyte: An open-source data integration platform that focuses on ELT. It provides a wide range of connectors for various data sources and destinations, and allows users to define transformation logic using SQL. See https://airbyte.com/</li> <li>Meltano: An open-source data integration platform that focuses on ELT. It provides a wide range of connectors for various data sources and destinations, and allows users to define transformation logic using yaml configuration files. See https://meltano.com/</li> <li>Dagster: An open-source data orchestrator for machine learning, analytics, and ETL. It provides a framework for building, scheduling, and monitoring data pipelines. See https://dagster.io/.</li> </ul>"},{"location":"0-introduction/#why-dlt","title":"Why <code>dlt</code>?","text":""},{"location":"0-introduction/#less-is-more","title":"Less is more","text":"<p>Or as per in the Zen of python: \"Simple is better than complex.\".</p> <p>Most of the alternatives mentioned above are powerful and flexible tools for data loading tasks. However, they can also be complex and require significant setup and configuration.</p> <p>For example, Meltano is very flexible through its YAML configuration files, but this also means that you need to learn its configuration syntax and structure. For Dagster you need to learn its framework and API, and host the service somewhere. For DLT, you need Python for the most part.</p>"},{"location":"0-introduction/#one-off-tasks","title":"One-off tasks","text":"<p>A <code>dlt</code> pipeline can be as simple as a single Python script that you can run from your local machine or a server. This makes it easy to set up and use for one-off data loading tasks, without the need for complex infrastructure or configuration.</p> <p>Once you get past the initial learning curve, it is easy to use.</p>"},{"location":"0-introduction/#environmentally-friendly","title":"Environmentally friendly","text":"<p>Depending on the size of your data and the frequency of your loads, using a lightweight tool like <code>dlt</code> can be more environmentally friendly than using a heavy-duty data integration platform that requires significant computational resources.</p> <p>Measure your carbon footprint</p> <p>See https://codecarbon.io/ if you wish to start measuring the carbon emissions of your code.</p>"},{"location":"0-introduction/#open-source","title":"Open source","text":"<p>Code has an Apache 2.0 license, so you can use it freely in your projects, even commercial ones. You can also contribute to the project if you wish to improve it or add new features.</p> <p>It has a <code>dlt+</code> version I have not explored nor needed yet, which seems to add more features and support.</p>"},{"location":"0-introduction/#cons-of-dlt","title":"Cons of <code>dlt</code>","text":"<p>I feel it could use more love in terms of</p> <ul> <li>CLI features: the CLI is clunky and at times feels useless or clunky</li> <li>Dashboards: <code>dlt</code> has some internals it use to maintain and track state. It also provides dashboards that offer an insight on these data. I haven't used or needed these dashboards but I can understand some people want them and use them.</li> </ul>"},{"location":"0-introduction/#style-in-documentation-is-not-consistent","title":"Style in documentation is not consistent","text":"<p>Writing documentation is difficult. I struggle with it myself. One of the verses of the Zen of Python claims:</p> <p>There should be one-- and preferably only one --obvious way to do it.</p> <p>The docs show multiple ways to achieve the same, which can be confusing for users. Sometimes imports are missing, some explanations on usage are unclear, and there are inconsistencies in the examples provided. It feels chaotic at times.</p>"},{"location":"1-getting-started/","title":"Getting Started with this Tutorial","text":"<p>I'm assuming you use Linux</p> <p>This tutorial assumes you are using a Linux-based operating system. While it may be possible to follow along on other operating systems, some commands and instructions may differ.</p>"},{"location":"1-getting-started/#using-docker-and-docker-compose","title":"Using Docker and Docker Compose","text":"<p>You'll need docker, docker-compose, python, and <code>uv</code> installed on your system to follow along with this tutorial.</p> <ol> <li>Install docker from https://docs.docker.com/get-docker/</li> <li>Install docker-compose from https://docs.docker.com/compose/install/</li> </ol>"},{"location":"1-getting-started/#what-if-i-dont-want-to-use-docker","title":"What if I don't want to use Docker?","text":"<p>You'll need to set up your local environment to run this tutorial. The following sections will guide you through the necessary steps.</p> <ol> <li>Install python using your preferred method. Here are some options:<ul> <li>From the official website: https://www.python.org/downloads/</li> <li>Using a package manager like <code>apt</code>, like <code>apt install python3</code> on Debian/Ubuntu</li> <li>Using a version manager like <code>pyenv</code>: https://github.com/pyenv/pyenv</li> <li>Using a version manager like <code>uv</code>: https://uv.io/docs/getting-started/installation/</li> </ul> </li> <li>Install <code>uv</code> by following the instructions at https://uv.io/docs/getting-started/installation/</li> </ol>"},{"location":"1-getting-started/#using-plain-python","title":"Using plain Python","text":"<p>This tutorial was written using <code>Python 3.12.3</code>. You can create a virtual environment using <code>venv</code> or <code>virtualenv</code> to isolate the dependencies for this tutorial.</p> <p>If you are using <code>uv</code>, you can create a virtual environment using <code>uv venv</code>:</p> <pre><code>uv venv --seed\nuv sync\nsource .venv/bin/activate\n</code></pre> <p>If you are using plain Python, you can create a virtual environment using <code>venv</code> from the standard library:</p> <pre><code>python3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n</code></pre> <p>requirements.txt</p> <p>The <code>requirements.txt</code> file was generated using <code>uv export --format requirements.txt -o requirements.txt</code> to ensure compatibility with <code>uv</code>.</p>"},{"location":"1-getting-started/#installing-duckdb","title":"Installing duckdb","text":"<p>Follow the instructions at https://duckdb.org/install/?platform=linux&amp;environment=cli to install the <code>duckdb</code> client on your system.</p>"},{"location":"1-getting-started/#setting-up-a-postgres-database","title":"Setting up a Postgres database","text":"<p>A Postgres database is required to run this tutorial. If you have docker compose installed, you can set up a local Postgres database using Docker and Docker Compose.</p> <pre><code>docker compose -f docker-compose.yml up db -d\n</code></pre> <p>This will start a Postgres database on <code>localhost:5555</code> with the following credentials (all defaults from the official documentation, but the password has been changed to <code>test</code>):</p> <ul> <li>User: <code>postgres</code></li> <li>Password: <code>test</code></li> <li>Database: <code>postgres</code></li> <li>Host: <code>localhost</code></li> <li>Port: <code>5555</code></li> </ul> <p>You can connect to this database using any Postgres client, such as <code>psql</code> or a GUI tool like DBeaver or pgAdmin. A shortcut connection is provided through <code>Makefile</code></p> <pre><code>make postgres.psql\n</code></pre>"},{"location":"1-getting-started/#setting-a-remote-postgres-database-using-neon","title":"Setting a remote Postgres database using Neon","text":"<p>You can set up an external Postgres database if you prefer.</p> <p>A free postgres instance can be created using services like https://neon.com/ (I'm not affiliated with them, it just seems less complicated).</p> <p>Create an account and get your connection string, and it should look something like this:</p> <pre><code>psql 'postgresql://neondb_owner:npg_some_long_url.aws.neon.tech/neondb?sslmode=require&amp;channel_binding=require'\n</code></pre> <p>Make sure that when generating the connection string, you deactivate connection pooling if that option is enabled.</p> <p></p> <p>You'll need this connection string, and we will see how to use it in the next sections.</p>"},{"location":"1-getting-started/#using-devcontainers","title":"Using devcontainers","text":"<p>A devcontainer is provided for this tutorial. If you are using VSCode and have docker installed, you can open this folder in a _devcontainer_and have everything set up for you. See https://code.visualstudio.com/docs/devcontainers/containers for more information on devcontainers.</p>"},{"location":"1-getting-started/#postgres-database-connection-settings-from-inside-the-devcontainer","title":"Postgres database connection settings from inside the devcontainer","text":"<p>If you are using the devcontainer, the Postgres database can be accessed at <code>postgres:5432</code> with the same credentials as above.</p>"},{"location":"2-our-first-pipeline/","title":"A simple example using <code>duckdb</code>","text":"<p>At its most basic form, <code>dlt</code> takes in an iterable that produces dictionaries. it will treat every dictionary as a unit of data and it will move it into a <code>target</code>.</p> <p>Take for example the following dictionary:</p> <pre><code>my_data = [\n    {\n        \"id\": 1,\n        \"name\": \"Mr. Mario\",\n        \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n        \"created_at\": \"2025-10-09 14:40:00\",\n        \"updated_at\": \"2025-10-09 14:50:00\",\n        \"metadata\": {\n            \"ingested_at\": dt.datetime.now().isoformat(),\n            \"script_name\": Path(__file__).name,\n        },\n    },\n    {\n        \"id\": 2,\n        \"name\": \"Mr. Luigi\",\n        \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n        \"created_at\": \"2025-10-08 16:15:00\",\n        \"updated_at\": \"2025-10-08 16:50:00\",\n        \"metadata\": {\n            \"ingested_at\": dt.datetime.now().isoformat(),\n            \"script_name\": Path(__file__).name,\n        },\n    },\n]\n</code></pre> <p>Here is a simple example that uses <code>dlt</code> to load data into a <code>duckdb</code> database.</p> <pre><code>    pipeline = dlt.pipeline(\n        pipeline_name=\"sample_pipeline\",\n        destination=dlt.destinations.duckdb,\n        dataset_name=\"sample_data\",\n    )\n    print(\"Running pipeline...\")\n\n    load_info = pipeline.run(\n        my_data,\n        table_name=\"samples\",\n        refresh=\"drop_sources\",\n        write_disposition={\n            \"disposition\": \"replace\",\n        },\n    )\n</code></pre> <p>dlt does not care how you produce the data</p> <p>The data source can be anything that produces dictionaries. For example, you can use <code>pandas</code>, e.g. <code>[row.to_dict() for _, row in df.iterrows()]</code>.</p>"},{"location":"2-our-first-pipeline/#what-is-happening-here","title":"What is happening here?","text":"<ol> <li>We define a list of dictionaries called <code>my_data</code>. Each dictionary represents a record with fields like <code>id</code>, <code>name</code>, <code>age</code>, and <code>address</code>.</li> <li>We create a <code>dlt</code> pipeline using <code>dlt.pipeline()</code>, specifying the pipeline name and the destination as <code>duckdb</code>.</li> <li>We use <code>pipeline.run(my_data)</code> to load the data from <code>my_data</code> into the <code>duckdb</code> database. The <code>write_disposition</code> parameter is set to <code>replace</code>, which means that if the target table already exists, it will be replaced with the new data. This is useful in this example to ensure that we start with a clean slate each time we run the script.</li> </ol>"},{"location":"2-our-first-pipeline/#running-the-example","title":"Running the example","text":"<p>Run the script directly using Python:</p> <pre><code>$ python dlt_tutorial/0_sample_pipeline_basic.py\nRunning pipeline...\nDone\n</code></pre> <p>DuckDB limitations on concurrency</p> <p>Beware that duckdb does not allow for concurrent access. If you try to run the pipeline while having a client already connected to the database, the pipeline will fail.</p> <p>This will create a <code>duckdb</code> database file in the current directory (by default named <code>sample_pipeline.duckdb</code>) and load the data into it. You can then query the database using any <code>duckdb</code> client or library.</p> <pre><code>$ duckdb sample_pipeline.duckdb -c \"select * from sample_data.samples;\"\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502   name    \u2502                 uuid                 \u2502        created_at        \u2502        updated_at        \u2502     metadata__ingested_at     \u2502   metadata__script_name    \u2502    _dlt_load_id    \u2502    _dlt_id     \u2502\n\u2502 int64 \u2502  varchar  \u2502               varchar                \u2502 timestamp with time zone \u2502 timestamp with time zone \u2502   timestamp with time zone    \u2502          varchar           \u2502      varchar       \u2502    varchar     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 Mr. Mario \u2502 a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc \u2502 2025-10-09 11:40:00-03   \u2502 2025-10-09 11:50:00-03   \u2502 2025-10-29 14:32:44.702763-03 \u2502 0_sample_pipeline_basic.py \u2502 1761769965.1842644 \u2502 sVJ0ooLEPZsP1Q \u2502\n\u2502     2 \u2502 Mr. Luigi \u2502 8c804ede-f8ae-409e-964d-9e355a3094e0 \u2502 2025-10-08 13:15:00-03   \u2502 2025-10-08 13:50:00-03   \u2502 2025-10-29 14:32:44.702794-03 \u2502 0_sample_pipeline_basic.py \u2502 1761769965.1842644 \u2502 P7QNYrq+IMHy1A \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"2-our-first-pipeline/#what-is-dlt-doing-under-the-hood","title":"What is <code>dlt</code> doing under the hood?","text":"<p>Congratulations! You have successfully loaded data into a <code>duckdb</code> database using <code>dlt</code>. There are a few things happening under the hood that are worth mentioning:</p> <ol> <li> <p>It created a <code>table</code> called <code>sample_data.samples</code>. In this table, every row has a <code>_dlt_load_id</code> and a <code>_dlt_id</code>. These are references to internal <code>dlt</code> mechanisms for tracking loads and identifying records.</p> </li> <li> <p><code>dlt</code> creates also three more tables that persist the state of the pipeline</p> <ol> <li><code>_dlt_loads</code></li> <li><code>_dlt_pipeline_state</code></li> <li><code>_dlt_version</code></li> </ol> </li> <li> <p>Unknown to you, <code>dlt</code> also created a directory in <code>~/.dlt/pipelines/sample_pipeline</code></p> </li> </ol>"},{"location":"2-our-first-pipeline/#wrapping-up","title":"Wrapping up","text":"<p>We saw a simple example of using <code>dlt</code> to load data into a <code>duckdb</code> database.</p> <p>We'll step it up a notch in the next part by using <code>resources</code> and <code>sources</code> to structure our data extraction better.</p>"},{"location":"2-our-first-pipeline/#full-example","title":"Full example","text":"<pre><code>import datetime as dt\nfrom pathlib import Path\nimport dlt\n\nmy_data = [\n    {\n        \"id\": 1,\n        \"name\": \"Mr. Mario\",\n        \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n        \"created_at\": \"2025-10-09 14:40:00\",\n        \"updated_at\": \"2025-10-09 14:50:00\",\n        \"metadata\": {\n            \"ingested_at\": dt.datetime.now().isoformat(),\n            \"script_name\": Path(__file__).name,\n        },\n    },\n    {\n        \"id\": 2,\n        \"name\": \"Mr. Luigi\",\n        \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n        \"created_at\": \"2025-10-08 16:15:00\",\n        \"updated_at\": \"2025-10-08 16:50:00\",\n        \"metadata\": {\n            \"ingested_at\": dt.datetime.now().isoformat(),\n            \"script_name\": Path(__file__).name,\n        },\n    },\n]\n\n\nif __name__ == \"__main__\":\n    pipeline = dlt.pipeline(\n        pipeline_name=\"sample_pipeline\",\n        destination=dlt.destinations.duckdb,\n        dataset_name=\"sample_data\",\n    )\n    print(\"Running pipeline...\")\n\n    load_info = pipeline.run(\n        my_data,\n        table_name=\"samples\",\n        refresh=\"drop_sources\",\n        write_disposition={\n            \"disposition\": \"replace\",\n        },\n    )\n\n    print(\"Done\")\n</code></pre>"},{"location":"3-sources-and-resources/","title":"Part 1: Using <code>resources</code> and <code>sources</code> to structure data extraction","text":"<p>In the previous part, we saw a simple example of using <code>dlt</code> to load data into a <code>duckdb</code> database. While that example was straightforward, we can do better.</p>"},{"location":"3-sources-and-resources/#a-primer-on-iterators-and-generators","title":"A primer on iterators and generators","text":"<p>An important concept in Python that <code>dlt</code> leverages is the idea of iterators and generators. These constructs allow for efficient looping over data without the need to load everything into memory at once.</p> <ul> <li>An iterator is an object that implements the iterator protocol, which consists of the methods <code>__iter__()</code> and <code>__next__()</code>. An iterator allows you to traverse through all the elements of a collection, such as a list or a set, one at a time.</li> <li>A generator is a special type of iterator that is defined using a function and the <code>yield</code> keyword. Generators allow you to produce a sequence of values over time, rather than computing them all at once and storing them in memory.</li> </ul> <p>Here's a simple example of a generator function that yields numbers from 0 to n-1:</p> <pre><code>def count_up_to(n):\n    count = 0\n    while count &lt; n:\n        yield count\n        count += 1\n</code></pre> <p>You can use this generator like this:</p> <pre><code>for number in count_up_to(5):\n    print(number)\n</code></pre> <p>This will output:</p> <pre><code>0\n1\n2\n3\n4\n</code></pre> <p>Generators are particularly useful when dealing with large datasets or streams of data, as they allow you to process one item at a time without loading everything into memory.</p> <p>However, Generators can only be iterated once. After they are exhausted, they cannot be reused. If you need to iterate over the data multiple times, you will need to create a new generator instance each time.</p> <pre><code>&gt;&gt;&gt; my_gen_5 = count_up_to(5)\n\n&gt;&gt;&gt; my_gen_5\n&lt;generator object count_up_to at 0x71aa29e4bb80&gt;\n\n&gt;&gt;&gt; [num for num in my_gen_5]\n[0, 1, 2, 3, 4]\n\n&gt;&gt;&gt; [num for num in my_gen_5]\n[]\n</code></pre>"},{"location":"3-sources-and-resources/#rewriting-our-previous-example-using-a-generator","title":"Rewriting our previous example using a generator","text":"<p>Let's rewrite our previous example using a generator function to produce the data. This will allow us to handle larger datasets more efficiently.</p> <pre><code>def sample_data() -&gt; Generator[dict, None, None]:\n    data = [\n        {\n            \"id\": 1,\n            \"name\": \"Mr. Mario\",\n            \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n            \"created_at\": \"2025-10-09 14:40:00\",\n            \"updated_at\": \"2025-10-09 14:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Mr. Luigi\",\n            \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n            \"created_at\": \"2025-10-08 16:15:00\",\n            \"updated_at\": \"2025-10-08 16:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n    ]\n    for item in data:\n        yield item\n</code></pre> <p>We can now pass this generator function to our <code>dlt</code> pipeline:</p> <pre><code>    pipeline = dlt.pipeline(\n        pipeline_name=\"sample_pipeline\",\n        destination=dlt.destinations.duckdb,\n        dataset_name=\"sample_data\",\n    )\n\n    print(\"Running pipeline...\")\n\n    load_info = pipeline.run(\n        sample_data,\n        table_name=\"samples\",\n        refresh=\"drop_sources\",\n        write_disposition={\n            \"disposition\": \"replace\",\n        },\n    )\n</code></pre> <p>This time, however, we are printing the result of the <code>pipeline.run()</code> method, which returns a <code>LoadInfo</code> object containing details about the loading process.</p> <pre><code>    print(\"Load info:\")\n    print(load_info)\n</code></pre> <p>You can run this script in the same way as before</p> <pre><code>$ python dlt_tutorial/1_sample_pipeline_basic.py\nRunning pipeline...\nDone\nLoad info:\nPipeline sample_pipeline load step completed in 0.28 seconds\n1 load package(s) were loaded to destination duckdb and into dataset sample_data\nThe duckdb destination used duckdb:////home/diego/Code/playground/dlt-tutorial/sample_pipeline.duckdb location to store data\nLoad package 1762003350.048731 is LOADED and contains no failed jobs\n</code></pre> <p>What is the load info?</p> <p>The load info provides details about the loading process, including the time taken, the number of load packages, and the destination where the data was stored. It also indicates whether there were any failed jobs during the loading process.</p> <p>Using generators with dlt</p> <p><code>dlt</code> works better if the generator yields dictionaries in batches, see https://dlthub.com/docs/reference/performance#yield-pages-instead-of-rows</p>"},{"location":"3-sources-and-resources/#using-resources-and-sources","title":"Using <code>resources</code> and <code>sources</code>","text":"<ul> <li>A resource is an (optionally async) function that yields data. To create a resource, we add the <code>@dlt.resource</code> decorator to that function.</li> <li>A source is a function decorated with <code>@dlt.source</code> that returns one or more resources.</li> </ul> <p>At its most basic implementation, a <code>resource</code> is implemented by decorating our data generator function with <code>@dlt.resource</code>:</p> <pre><code>@dlt.resource\ndef sample_data() -&gt; Generator[dict, None, None]:\n    my_data = [\n        {\n            \"id\": 1,\n            \"name\": \"Mr. Mario\",\n            \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n            \"created_at\": \"2025-10-09 14:40:00\",\n            \"updated_at\": \"2025-10-09 14:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Mr. Luigi\",\n            \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n            \"created_at\": \"2025-10-08 16:15:00\",\n            \"updated_at\": \"2025-10-08 16:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n    ]\n    for item in my_data:\n        yield item\n</code></pre> <p>A <code>source</code> is created by defining a function that returns the resource:</p> <pre><code>@dlt.source\ndef sample_source():\n    yield sample_data\n</code></pre> <p>We can then use this source in our pipeline:</p> <pre><code>    pipeline = dlt.pipeline(\n        pipeline_name=\"sample_pipeline\",\n        destination=dlt.destinations.duckdb,\n        dataset_name=\"sample_data\",\n    )\n\n    load_info = pipeline.run(\n        sample_source,\n        table_name=\"samples\",\n        refresh=\"drop_sources\",\n        write_disposition={\n            \"disposition\": \"replace\",\n        },\n    )\n\n    print(load_info)\n</code></pre>"},{"location":"3-sources-and-resources/#what-changed","title":"What changed?","text":"<p><code>dlt</code> automatically generates configuration specs for functions decorated with <code>@dlt.source</code>, <code>@dlt.resource</code>, and <code>@dlt.destination</code></p> <p>For example, it allows now for injecting configuration values</p>"},{"location":"3-sources-and-resources/#injecting-configuration-values","title":"Injecting configuration values","text":"<p>We can modify our source function to accept a configuration parameter:</p> <pre><code>@dlt.source\ndef sample_source(my_custom_parameter: str = \"foo\"):\n    print(f\"Custom parameter value: {my_custom_parameter}\")\n    yield sample_data\n</code></pre> <p>This parameter can now be set using environment variables, configuration files, or command-line arguments when running the pipeline.</p> <pre><code>$ MY_CUSTOM_PARAMETER=\"pythonchile\" python dlt_tutorial/2b_sample_pipeline_sources_resources_with_config.py\nCustom parameter value: pythonchile\nRunning pipeline...\nDone\n...\n</code></pre> <p>Alternatively, we can set the parameter in a configuration file located at <code>.dlt/config.toml</code>:</p> <pre><code>[sample_pipeline]\nmy_custom_parameter = \"baz\"\n</code></pre> <p>Running the pipeline now will use the value from the configuration file:</p> <pre><code>$ python dlt_tutorial/2b_sample_pipeline_sources_resources_with_config.py\nCustom parameter value: baz\nRunning pipeline...\nDone\n...\n</code></pre> <p>dlt searches for values in multiple places in an specific order</p> <p>See https://dlthub.com/docs/general-usage/credentials/setup#how-dlt-looks-for-values for more information on how <code>dlt</code> searches for configuration values.</p> <p>We will be using <code>resource</code> directly from now on</p> <p>From now on, we will be using <code>resource</code> directly instead of wrapping them in sources, unless we need to group multiple resources together.</p> <p>Since we are only using a single resource, we can pass it directly to the <code>pipeline.run()</code> method.</p>"},{"location":"3-sources-and-resources/#wrapping-up","title":"Wrapping up","text":"<p>In this part, we learned about iterators and generators in Python, and how they can be used to efficiently handle data in <code>dlt</code> pipelines. We also introduced the concepts of <code>resources</code> and <code>sources</code>, which help structure data extraction in a more modular way.</p> <p>In the next part, we will change to a postgres destination and see how to handle more complex loading patterns.</p>"},{"location":"4-switching-to-postgres/","title":"Part 2: Implementing incremental data loading","text":""},{"location":"4-switching-to-postgres/#introduction-to-incremental-loading","title":"Introduction to incremental loading","text":"<p>Consider the following state of your table at a given time:</p> id name created_at updated_at 1 Mr. Mario 2025-10-09 14:40:00 2025-10-09 14:50:00 2 Mr. Luigi 2025-10-08 16:15:00 2025-10-08 16:50:00 <p>In this table, we can identify two records, which can be uniquely identified by their <code>id</code> or <code>uuid</code> fields. Each record also has <code>created_at</code> and <code>updated_at</code> timestamps.</p> <p>Now, imagine that some time has passed, and we have new data to load into our table. We fetch our data from a remote API that serves incremental updates on information about Mario and Luigi.</p> <p>The new incoming data looks like this:</p> id name created_at updated_at 1 Jumpman 2025-10-09 14:40:00 2025-10-10 11:50:00 3 Ms. Peach 2025-10-12 13:15:00 2025-10-13 13:50:00 <p>Mario is now known as \"Jumpman\", it was updated after its latest update, and a new record for \"Ms. Peach\" has been added.</p> <p>What should be the final state of our table after loading this new data? It depends on the logic we want to implement for handling updates and inserts.</p> <p>Although there are many strategies, here are some common ones:</p>"},{"location":"4-switching-to-postgres/#replace-everything","title":"Replace everything","text":"<p>Ignore existing state and replace everything: In this case, we would simply replace the entire table with the incoming data.</p> id name created_at updated_at 1 Jumpman 2025-10-09 11:40:00-03 2025-10-10 11:50:00-03 3 Ms. Peach 2025-10-10 13:25:00-03 2025-10-10 13:50:00-03"},{"location":"4-switching-to-postgres/#append-new-data","title":"Append new data","text":"<p>Simply append the incoming data to the existing table:</p> id name created_at updated_at 1 Mr. Mario 2025-10-09 11:40:00-03 2025-10-09 11:50:00-03 2 Mr. Luigi 2025-10-08 13:15:00-03 2025-10-08 13:50:00-03 1 Jumpman 2025-10-09 11:40:00-03 2025-10-10 11:50:00-03 3 Ms. Peach 2025-10-10 13:25:00-03 2025-10-10 13:50:00-03"},{"location":"4-switching-to-postgres/#merge-new-records-into-existing-ones","title":"Merge new records into existing ones","text":"<p>We can update existing records or append new ones. For this, we need to rely on some identifier such as the <code>id</code>, that uniquely identifies a record:</p> <p>If we consider the <code>id</code> as such a record, we would update Mario's record with the new name and updated timestamp, and insert the new record for \"Ms. Peach\".</p> id name created_at updated_at 1 Jumpman 2025-10-09 11:40:00-03 2025-10-10 11:50:00-03 2 Mr. Luigi 2025-10-08 13:15:00-03 2025-10-08 13:50:00-03 3 Ms. Peach 2025-10-10 13:25:00-03 2025-10-10 13:50:00-03 <p>The merge strategy is the most common one when dealing with incremental data loading. It allows us to keep our data up-to-date while minimizing the amount of data we need to process.</p>"},{"location":"4-switching-to-postgres/#an-example-with-postgres","title":"An example with Postgres","text":"<p>A good example of a database that supports incremental loading is Postgres. Postgres provides several mechanisms for handling updates and inserts, such as <code>UPSERT</code> (using <code>ON CONFLICT</code>), or <code>MERGE</code> in more recent versions, which allows us to insert new records or update existing ones based on a unique constraint.</p> <p>As per its documentation, <code>MERGE</code> implements a join operation using a <code>join_condition</code>.</p> <pre><code>[ WITH with_query [, ...] ]\nMERGE INTO [ ONLY ] target_table_name [ * ] [ [ AS ] target_alias ]\n    USING data_source ON join_condition\n    when_clause [...]\n    [ RETURNING [ WITH ( { OLD | NEW } AS output_alias [, ...] ) ]\n                { * | output_expression [ [ AS ] output_name ] } [, ...] ]\n</code></pre> <p>In the following example, <code>MERGE</code> requires a <code>target</code> table (the table we want to update) and a <code>source</code> table (the new data we want to insert or use to update the target).</p> <pre><code>MERGE INTO target_table tt\n  USING source_table st\n    ON st.id = tt.id\nWHEN MATCHED THEN\n  UPDATE SET name = st.name\nWHEN NOT MATCHED THEN\n  INSERT (id, name)\n  VALUES (st.id, st.name);\n</code></pre> <p>In this example, we are merging data from <code>source_table</code> into <code>target_table</code> based on the <code>id</code> column. If a matching record is found, we update the <code>name</code> column; if not, we insert a new record.</p>"},{"location":"4-switching-to-postgres/#implementing-incremental-loading-with-dlt","title":"Implementing incremental loading with <code>dlt</code>","text":"<p>How do we know what scenario we need to implement? It depends. <code>dlt</code> summarizes every use case with the following diagram:</p> <p></p> <p>We can load data in different ways, depending on the requirements. See the documentation for a full introduction</p> <p>We can define the write disposition and the write strategy when running a pipeline. We can choose from <code>\"replace\"</code>, <code>\"append\"</code>, <code>\"merge\"</code></p> <ol> <li><code>\"replace\"</code>: Replaces the table entirely</li> <li><code>\"append\"</code>: Loads rows incrementally, regardless of their values</li> <li> <p><code>\"merge\"</code>: Inserts only rows that are relevant to the update, e.g. that match data in target according to some strategy and some key that allows for identifying matching rows. For the \"merge\" disposition</p> <ol> <li><code>\"delete-insert\"</code>: given a match between incoming and existing rows on some key, DELETE the target row and INSERT it from incoming. This operation is locking or it may not be supported, depending on the target database.</li> <li><code>\"upsert\"</code>: given a match between incoming and existing rows on some key, UPDATE the target row and \"INSERT\" what's changed from incoming data. This operation is not locking but it may not be supported, depending on the target database.</li> <li><code>\"scd2\"</code>: given a match between incoming and existing rows on some key, leave the existing target row and \"INSERT\" a new one from the incoming data. Using some additional columns, this allows for tracking the validity of the latest value, but it takes more space in disk.</li> </ol> </li> </ol>"},{"location":"4-switching-to-postgres/#setting-up-a-postgres-database","title":"Setting up a Postgres database","text":"<p>Refer to the Getting Started section to learn how to set up a postgres database. Once you have a valid postgres instance, you need to setup its postgres credentials.</p> <p>Some incremental loading patterns are not supported by <code>duckdb</code></p> <p>Duckdb is an analytics, in-memory database. It shines for some types of tasks, and it offers its own implementation of <code>MERGE</code>. However, incremental loading patterns are not fully supported by <code>dlt</code> at the moment.</p>"},{"location":"4-switching-to-postgres/#configure-dlt-to-use-postgres","title":"Configure <code>dlt</code> to use Postgres","text":"<p>You'll need to tell <code>dlt</code> how to connect to your Postgres database. The recommended way is to create a <code>secrets.toml</code> file in the root of this project with the following content:</p> <pre><code>[sample_pipeline_postgres.destination.postgres.credentials]\nhost = \"localhost\"\nport = 5555\nuser = \"postgres\"\npassword = \"test\"\ndbname = \"postgres\"\n</code></pre> <p>If you wish to use the connection string method, for example when using a neon database, you can also do it like this:</p> <pre><code>sample_pipeline_postgres.destination.postgres.credentials = 'postgresql://neondb_owner:&lt;password&gt;@&lt;host&gt;/neondb?sslmode=require&amp;channel_binding=require'\n</code></pre> <p>Do not commit your secrets.toml file</p> <p>Make sure to add <code>secrets.toml</code> to your <code>.gitignore</code> file to avoid committing sensitive information to version control.</p> <p>Adjust connection parameters as needed</p> <p>Adjust the connection parameters (<code>host</code>, <code>port</code>, <code>user</code>, <code>password</code>, <code>dbname</code>) according to your Postgres setup. If you are using the devcontainer, the host should be <code>postgres</code> and the port <code>5432</code>.</p> <p>Set connection parameters using environment variables</p> <p>You can also set the connection parameters using environment variables. See the documentation for more information.</p>"},{"location":"4-switching-to-postgres/#replace-everything-using-postgres","title":"Replace everything using postgres","text":"<p>You can run now the <code>3_sample_pipeline_postgres_config.py</code> script to test the Postgres connection and configuration.</p> <p>The only things that have changed from the previous examples is the <code>write_disposition</code> and <code>write_strategy</code> parameters when running the pipeline:</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    write_disposition={\n        \"disposition\": \"replace\",\n    },\n)\ndef sample_data() -&gt; Generator[dict, None, None]:\n    my_data = [\n        {\n            \"id\": 1,\n            \"name\": \"Mr. Mario\",\n            \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n            \"created_at\": \"2025-10-09 14:40:00\",\n            \"updated_at\": \"2025-10-09 14:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Mr. Luigi\",\n            \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n            \"created_at\": \"2025-10-08 16:15:00\",\n            \"updated_at\": \"2025-10-08 16:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n    ]\n    for item in my_data:\n        yield item\n</code></pre> <p>If we run this example, we should see output similar to this:</p> <pre><code>$ python dlt_tutorial/3_sample_pipeline_postgres_config.py\nStarting pipeline...\nPipeline run completed.\nPipeline sample_pipeline_postgres load step completed in 0.11 seconds\n1 load package(s) were loaded to destination postgres and into dataset sample_data\nThe postgres destination used postgresql://postgres:***@localhost:5555/postgres location to store data\nLoad package 1762003276.163356 is LOADED and contains no failed jobs\n</code></pre> <p>We can now connect to our Postgres database and check the contents of the <code>sample_data.samples</code> table:</p> <pre><code>$PGPASSWORD=test psql -h 0.0.0.0 -p 5555 -U postgres --pset expanded=auto -c \"select * from sample_data.samples;\"\n id |   name    |                 uuid                 |       created_at       |       updated_at       |     metadata__ingested_at     |        metadata__script_name         |   _dlt_load_id    |    _dlt_id     \n----+-----------+--------------------------------------+------------------------+------------------------+-------------------------------+--------------------------------------+-------------------+----------------\n  1 | Mr. Mario | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-09 14:50:00+00 | 2025-11-01 10:21:16.172598+00 | 3_sample_pipeline_postgres_config.py | 1762003276.163356 | MJaJ6AzyVleWlQ\n  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-01 10:21:16.172663+00 | 3_sample_pipeline_postgres_config.py | 1762003276.163356 | IrYyUJd1NAmnBQ\n(2 rows)\n</code></pre> What happens if we run the script again? <p>If you run the script again, since the <code>write_disposition</code> is set to <code>\"replace\"</code> and the <code>refresh</code> parameter is set to <code>\"drop_sources\"</code>, the existing data in the <code>sample_data.samples</code> table will be replaced with the new data fetched from the source, every time. You should see different <code>metadata__ingested_at</code> timestamps, and different <code>_dlt_load_id</code> and <code>_dlt_id</code> values with each run.</p>"},{"location":"5-incremental-loading/","title":"Trying out incremental loading","text":""},{"location":"5-incremental-loading/#a-slight-detour-enabling-full-refresh-through-command-line-arguments","title":"A slight detour: enabling full refresh through command line arguments","text":"<p>We will explore incremental loading strategies in the next sections, but first, we are going to need an easy way to tell dlt how to handle full refreshes so we can rapidly start from scratch, if something goes wrong.</p> <p>Refresh strategy tells dlt if it should drop existing data in the destination before loading new data. Available strategies are, as per the documentation:</p> <ul> <li><code>drop_sources</code> - Drop tables and source and resource state for all sources currently being processed in run or extract methods of the pipeline. (Note: schema history is erased)</li> <li><code>drop_resources</code>- Drop tables and resource state for all resources being processed. Source level state is not modified. (Note: schema history is erased)</li> <li><code>drop_data</code> - Wipe all data and resource state for all resources being processed. Schema is not modified.</li> </ul> <pre><code>    pipeline = dlt.pipeline(\n        pipeline_name=\"sample_pipeline_postgres\",\n        destination=dlt.destinations.postgres,\n        dataset_name=\"sample_data\",\n    )\n\n    print(\"Starting pipeline...\")\n    load_info = pipeline.run(\n        sample_data,\n        table_name=\"samples\",\n        refresh=\"drop_sources\",\n    )\n</code></pre> <p>To enable this option we can modify our pipeline script to include the <code>refresh</code> parameter when creating the pipeline.</p> <pre><code>def parse_args():\n    parser = argparse.ArgumentParser(description=\"Sample DLT Pipeline with Append\")\n    parser.add_argument(\n        \"--refresh\",\n        action=\"store_true\",\n        help=\"Refresh the data in the destination (if applicable)\",\n    )\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    should_refresh = args.refresh\n    pipeline = dlt.pipeline(\n        pipeline_name=\"sample_pipeline_postgres\",\n        destination=dlt.destinations.postgres,\n        dataset_name=\"sample_data\",\n    )\n    print(\"Running pipeline...\")\n    refresh_mode: TRefreshMode = \"drop_sources\"\n\n    if should_refresh:\n        print(\"Refreshing data in the destination.\")\n\n    load_info = pipeline.run(\n        sample_data,\n        table_name=\"samples\",\n        refresh=refresh_mode if should_refresh else None,\n    )\n</code></pre> <p>This way, when we run our pipeline with the <code>--refresh</code> flag, it will drop existing data in the destination before loading new data.</p> <p>We also implement the parameter to simulate loading new data in the next sections. We modify our <code>resource</code> based on this flag.</p> <pre><code>def sample_data(use_new_data: bool = False) -&gt; Generator[dict, None, None]:\n    my_data = [\n        {\n            \"id\": 1,\n            \"name\": \"Mr. Mario\",\n            \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n            \"created_at\": \"2025-10-09 14:40:00\",\n            \"updated_at\": \"2025-10-09 14:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Mr. Luigi\",\n            \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n            \"created_at\": \"2025-10-08 16:15:00\",\n            \"updated_at\": \"2025-10-08 16:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n    ]\n\n    if use_new_data:\n        print(\"Using new data for this run.\")\n        my_data = [\n            {\n                \"id\": 1,\n                \"name\": \"Jumpman\",\n                \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n                \"created_at\": \"2025-10-09 14:40:00\",\n                \"updated_at\": \"2025-10-10 11:50:00\",\n                \"metadata\": {\n                    \"ingested_at\": dt.datetime.now().isoformat(),\n                    \"script_name\": Path(__file__).name,\n                },\n            },\n            {\n                \"id\": 3,\n                \"name\": \"Ms. Peach\",\n                \"uuid\": \"1a73f32f-9144-4318-9a00-4437bde41627\",\n                \"created_at\": \"2025-10-12 13:15:00\",\n                \"updated_at\": \"2025-10-13 13:50:00\",\n                \"metadata\": {\n                    \"ingested_at\": dt.datetime.now().isoformat(),\n                    \"script_name\": Path(__file__).name,\n                },\n            },\n        ]\n    for item in my_data:\n        yield item\n</code></pre> <p>Our new command-line interface looks like this:</p> <pre><code>$ python dlt_tutorial/4_sample_pipeline_append.py --help\nusage: 4_sample_pipeline_append.py [-h] [--refresh]\n\nSample DLT Pipeline with Append\n\noptions:\n  -h, --help  show this help message and exit\n  --refresh   Refresh the data in the destination (if applicable)\n</code></pre> <p>and it accepts a parameter through which we can simulate loading new data:</p> <pre><code>USE_NEW_DATA=1 python dlt_tutorial/4_sample_pipeline_append.py\n</code></pre>"},{"location":"5-incremental-loading/#append-only","title":"Append only","text":"<p>You can now run the pipeline with the <code>--refresh</code> flag to start from scratch:</p> <pre><code>$ python dlt_tutorial/4_sample_pipeline_append.py --refresh\nRunning pipeline...\nCustom parameter value: foo\nPipeline sample_pipeline_postgres load step completed in 0.09 seconds\n1 load package(s) were loaded to destination postgres and into dataset sample_data\nThe postgres destination used postgresql://postgres:***@localhost:5555/postgres location to store data\nLoad package 1762043727.4561055 is LOADED and contains no failed jobs\nDone\n</code></pre> <p>and run it again without the <code>--refresh</code> flag to append new data:</p> <pre><code>$ python dlt_tutorial/4_sample_pipeline_append.py --new-data\nRunning pipeline...\nCustom parameter value: foo\nPipeline sample_pipeline_postgres load step completed in 0.08 seconds\n1 load package(s) were loaded to destination postgres and into dataset sample_data\nThe postgres destination used postgresql://postgres:***@localhost:5555/postgres location to store data\nLoad package 1762044000.123456 is LOADED and contains no failed jobs\nDone\n</code></pre> <p>You can check the contents of the <code>sample_data</code> table in Postgres to see the results:</p> <pre><code>SELECT * FROM sample_data.samples;\n</code></pre> What happens if we run the pipeline multiple times without the <code>--refresh</code> flag? <p>Each time we run the pipeline, new data will be appended to the existing data in the destination. This is because we are using the default <code>append</code> loading strategy, which adds new records to the existing table without modifying or deleting any existing records.</p> <pre><code> id |   name    |                 uuid                 |       created_at       |       updated_at       |     metadata__ingested_at     |    metadata__script_name    |    _dlt_load_id    |    _dlt_id     \n----+-----------+--------------------------------------+------------------------+------------------------+-------------------------------+-----------------------------+--------------------+----------------\n  1 | Mr. Mario | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-09 14:50:00+00 | 2025-11-02 18:30:16.036039+00 | 4_sample_pipeline_append.py | 1762119016.0306315 | pJA1hF4HneOUbw\n  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-02 18:30:16.036077+00 | 4_sample_pipeline_append.py | 1762119016.0306315 | UxcgfhkMgMleKg\n  1 | Mr. Mario | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-09 14:50:00+00 | 2025-11-02 18:30:20.517005+00 | 4_sample_pipeline_append.py | 1762119020.5093243 | HMDNsEtH+RBPXQ\n  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-02 18:30:20.517057+00 | 4_sample_pipeline_append.py | 1762119020.5093243 | SbekuWD7fosP1Q\n(4 rows)\n</code></pre>"},{"location":"5-incremental-loading/#append-with-incremental-primary-key","title":"Append with incremental primary key","text":"<p>In our previous example, we were able to append new data to the destination, but we did not have a way to uniquely identify each record. This can lead to duplicate records if the same data is loaded multiple times. An alternative is to use an incremental primary key to uniquely identify each record.</p> <p>This can be done in two steps:</p> <ol> <li> <p>Modify the resource decorator to specify the <code>id</code> column as a primary key.</p> <pre><code>@dlt.resource(primary_key=\"id\", write_disposition=\"append\")\ndef sample_data(use_new_data: bool = False) -&gt; Generator[dict, None, None]:\n</code></pre> </li> <li> <p>Apply hints to the resource to specify that the <code>id</code> column should be treated as an incremental primary key. <code>dlt</code> allows this by using the <code>apply_hints</code> method on the resource.</p> <pre><code>    # add unique and incremental primary key on \"id\" column\n    hinted_data = sample_data.apply_hints(incremental=dlt.sources.incremental(\"id\"))\n\n    load_info = pipeline.run(\n        hinted_data,\n        table_name=\"samples\",\n        refresh=refresh_mode if should_refresh else None,\n    )\n</code></pre> </li> </ol> <p>See more about schema hints in the documentation.</p> <p>incremental hint only filters from incoming data</p> <p><code>dlt.sources.incremental</code> is recommended when you want to reduce the amount of data extracted from your source by only selecting new or updated data since your last data extraction.</p> <p>You can now run the modified pipeline with the <code>--refresh</code> flag to start from scratch:</p> <pre><code>$ python dlt_tutorial/4b_sample_pipeline_append_pk.py --refresh\n# output omitted for brevity\n</code></pre> <p>and run it again without the <code>--refresh</code> flag to append new data:</p> <pre><code>$ python dlt_tutorial/4b_sample_pipeline_append_pk.py\n# output omitted for brevity\n0 load package(s) were loaded to destination postgres and into dataset None\nThe postgres destination used postgresql://postgres:***@localhost:5555/postgres location to store data\nDone\n</code></pre> <p>You'll see that no new records were added to the <code>sample_data</code> table. You can check the contents of the <code>sample_data</code> table in Postgres to see the results:</p> <pre><code>SELECT * FROM sample_data.samples;\n</code></pre> What happens if we run the pipeline multiple times without the <code>--refresh</code> flag? <p>Each time we run the pipeline, new data will be appended to the existing data in the destination. However, since we have specified the <code>id</code> column as a primary key, <code>dlt</code> will ensure that there are no duplicate records based on this key. If a record with the same <code>id</code> already exists in the destination, incoming data with the same <code>id</code> will be discarded.</p> <p>Try running the pipeline now passing the <code>USE_NEW_DATA=1</code> environment variable to simulate loading new data:</p> <pre><code>$ USE_NEW_DATA=1 python dlt_tutorial/4b_sample_pipeline_append_pk.py\n# output omitted for brevity\n</code></pre> What is the result of running the pipeline with <code>USE_NEW_DATA=1</code>? <p>When running the pipeline with <code>USE_NEW_DATA=1</code>, the resource function generates a new set of data that includes records with <code>id</code> values that already exist in the destination. However, since we have specified the <code>id</code> column as a primary key and applied the incremental hint, <code>dlt</code> will discard any incoming records that have an <code>id</code> that already exists in the destination. As a result, only new records with unique <code>id</code> values will be appended to the destination.</p> <pre><code> id |   name    |                 uuid                 |       created_at       |       updated_at       |     metadata__ingested_at     |      metadata__script_name      |    _dlt_load_id    |    _dlt_id     \n----+-----------+--------------------------------------+------------------------+------------------------+-------------------------------+---------------------------------+--------------------+----------------\n  1 | Mr. Mario | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-09 14:50:00+00 | 2025-11-02 15:23:52.179837+00 | 4b_sample_pipeline_append_pk.py | 1762107832.1758425 | 5kO1pi9oN0z/vQ\n  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-02 15:23:52.179866+00 | 4b_sample_pipeline_append_pk.py | 1762107832.1758425 | BpMmoBJH+6WA/A\n  3 | Ms. Peach | 1a73f32f-9144-4318-9a00-4437bde41627 | 2025-10-12 13:15:00+00 | 2025-10-13 13:50:00+00 | 2025-11-02 15:24:00.04396+00  | 4b_sample_pipeline_append_pk.py | 1762107840.0396352 | 4ZUmjQ7Pt9N/uQ\n(3 rows)\n</code></pre>"},{"location":"5-incremental-loading/#merge-strategies","title":"Merge strategies","text":"<p><code>dlt</code> also supports <code>merge</code> strategies, which allow you to update existing records in the destination based on a specified key.</p> <p>The merge write disposition can be used with three different strategies:</p> <ol> <li><code>delete-insert</code>: The delete-insert strategy loads data to a staging dataset, deduplicates the staging data if a primary_key is provided, deletes the data from the destination using merge_key and primary_key, and then inserts the new records.</li> <li><code>upsert</code>: update record if key exists in target table, or insert record if key does not exist in target table</li> <li><code>scd2</code>: Slowly Changing Dimensions type 2 strategy, which tracks historical changes in data by creating new records for each change while preserving previous records. It will add a new column to track the validity period of each record.</li> </ol> <p>To implement these strategies, we need to modify the <code>write_disposition</code> parameter when creating the pipeline.</p>"},{"location":"5-incremental-loading/#upsert","title":"Upsert","text":"<p>We can remove the <code>apply_hints</code> method since we are not using an incremental primary key in this example.</p> <p>If we want to use the <code>upsert</code> strategy, we can run the modified pipeline with the <code>--refresh</code> flag to start from scratch:</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition={\"disposition\": \"merge\", \"strategy\": \"upsert\"},\n)\ndef sample_data(use_new_data: bool = False) -&gt; Generator[dict, None, None]:\n</code></pre> <pre><code>$ python dlt_tutorial/5_sample_pipeline_merge_upsert.py --refresh\n# output omitted for brevity\n</code></pre> <p>and run it again without the <code>--refresh</code> flag to upsert new data:</p> <pre><code>$ python dlt_tutorial/5_sample_pipeline_merge_upsert.py\n# output omitted for brevity\n</code></pre> <p>You should see that no records were added to the <code>sample_data</code> table. You can check the contents of the <code>sample_data</code> table in Postgres to see the results:</p> <pre><code>SELECT * FROM sample_data;\n</code></pre> <p>Try now running the pipeline passing the <code>USE_NEW_DATA=1</code> environment variable to simulate loading new data:</p> <pre><code>$ USE_NEW_DATA=1 python dlt_tutorial/5_sample_pipeline_merge_upsert.py\n# output omitted for brevity\n</code></pre> What is the result of running the pipeline with <code>USE_NEW_DATA=1</code>? <p>When running the pipeline with <code>USE_NEW_DATA=1</code>, the resource function generates a new set of data that includes records with <code>id</code> values that already exist in the destination. Since we are using the <code>upsert</code> strategy, <code>dlt</code> will update existing records in the destination if a record with the same <code>id</code> already exists, or insert new records if the <code>id</code> does not exist.</p> <pre><code>postgres=# select * from sample_data.samples;\n id |   name    |                 uuid                 |       created_at       |       updated_at       |     metadata__ingested_at     |       metadata__script_name       |    _dlt_load_id    |_dlt_id\n----+-----------+--------------------------------------+------------------------+------------------------+-------------------------------+-----------------------------------+--------------------+----------------\n  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-02 18:37:17.901958+00 | 5_sample_pipeline_merge_upsert.py | 1762119437.8970616 | M6/KlLzJ2FeV/w\n  1 | Jumpman   | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-10 11:50:00+00 | 2025-11-02 18:37:23.170644+00 | 5_sample_pipeline_merge_upsert.py | 1762119443.1653655 | z5jIxXIbUnmbKw\n  3 | Ms. Peach | 1a73f32f-9144-4318-9a00-4437bde41627 | 2025-10-12 13:15:00+00 | 2025-10-13 13:50:00+00 | 2025-11-02 18:37:23.170657+00 | 5_sample_pipeline_merge_upsert.py | 1762119443.1653655 | hKdQ/VhGat+Pgw\n(3 rows)\n</code></pre>"},{"location":"5-incremental-loading/#slowly-changing-dimensions-scd2","title":"Slowly Changing Dimensions (SCD2)","text":"<p>In the same way, we can implement the <code>scd2</code> strategy by modifying the <code>write_disposition</code> parameter when creating the pipeline.</p> <p>Given a match between incoming and existing rows on some key, leave the existing target row and \"INSERT\" a new record from the incoming data, using some auxiliary columns.</p> <p>This allows for tracking the validity of the latest value, but it takes more space in disk.</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition={\"disposition\": \"merge\", \"strategy\": \"scd2\"},\n)\ndef sample_data(use_new_data: bool = False) -&gt; Generator[dict, None, None]:\n</code></pre> <p>In practice, <code>dlt</code> will calculate a surrogate key for each record based on the primary key and the hash of the record's content.</p> <p>When a record with the same primary key but different content is encountered, a new record is inserted with a new surrogate key, while the existing record is marked as expired.</p> <p>You can run the modified pipeline with the <code>--refresh</code> flag to start from scratch:</p> <pre><code>$ python dlt_tutorial/6_sample_pipeline_merge_scd2.py --refresh\n# output omitted for brevity\n</code></pre> <p>and run it again without the <code>--refresh</code> flag to upsert new data:</p> <pre><code>$ python dlt_tutorial/6_sample_pipeline_merge_scd2.py\n# output omitted for brevity  \n</code></pre> <p>Since our data has a metadata column named <code>metadata__ingested_at</code> that is based on the execution timestamp, <code>dlt</code> will compute a different surrogate key every time a record is inserted.</p> <p>This will in effect insert new rows every time we run the pipeline, and will mark the previous rows as expired.</p> <pre><code>postgres=# select * from sample_data.samples;\n        _dlt_valid_from        |        _dlt_valid_to         | id |   name    |                 uuid                 |       created_at       |       updated_at       |     metadata__ingested_at     |      metadata__script_name      |    _dlt_load_id    |    _dlt_id     \n-------------------------------+------------------------------+----+-----------+--------------------------------------+------------------------+------------------------+-------------------------------+---------------------------------+--------------------+----------------\n 2025-11-02 21:43:43.942528+00 | 2025-11-02 21:46:07.88226+00 |  1 | Mr. Mario | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-09 14:50:00+00 | 2025-11-02 18:43:43.948416+00 | 6_sample_pipeline_merge_scd2.py | 1762119823.9425282 | 2PDbMZWckGbEzQ\n 2025-11-02 21:43:43.942528+00 | 2025-11-02 21:46:07.88226+00 |  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-02 18:43:43.94847+00  | 6_sample_pipeline_merge_scd2.py | 1762119823.9425282 | zFQAhPCh1tzs2A\n 2025-11-02 21:46:07.88226+00  |                              |  1 | Mr. Mario | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-09 14:50:00+00 | 2025-11-02 18:46:07.887446+00 | 6_sample_pipeline_merge_scd2.py | 1762119967.8822596 | I6WPZYVDBhg9zQ\n 2025-11-02 21:46:07.88226+00  |                              |  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-02 18:46:07.887478+00 | 6_sample_pipeline_merge_scd2.py | 1762119967.8822596 | CrpVG8J0ezqiQg\n(4 rows)\n</code></pre> How do we know what is the most recent value when using SCD2? <p>When using the SCD2 strategy, each record has two additional columns: <code>_dlt_valid_from</code> and <code>_dlt_valid_to</code>. The <code>_dlt_valid_from</code> column indicates the timestamp when the record became valid, while the <code>_dlt_valid_to</code> column indicates the timestamp when the record was superseded by a newer version.</p> <p>To find the most recent value for a given primary key, you can query for records where the <code>_dlt_valid_to</code> column is <code>NULL</code>, as this indicates that the record is currently valid.</p> <p>For example, to find the most recent records in the <code>sample_data.samples</code> table, you can run the following SQL query:</p> <pre><code>SELECT * FROM sample_data.samples WHERE _dlt_valid_to IS NULL;\n</code></pre> <p>This will return only the records that are currently valid, allowing you to see the most recent values for each primary key.</p> What happens if you execute the pipeline using the environment variable <code>USE_NEW_DATA=1</code>? <p>In the new data you will see that <code>Mr. Luigi</code> is not present. This means that when executing the pipeline with <code>USE_NEW_DATA=1</code>, this record will be marked as expired in the destination table (A hard delete), and a new record for <code>Jumpman</code> will be inserted.</p>"},{"location":"5-incremental-loading/#wrapping-up","title":"Wrapping up","text":"<p>We've explored different incremental loading strategies using <code>dlt</code>, including <code>append</code>, <code>upsert</code>, and <code>SCD2</code>. Each strategy has its own use cases and benefits, depending on the requirements of your data pipeline.</p> <p><code>dlt</code> offers more strategies and options for incremental loading. Refer to the dlt documentation for more information on how to implement these strategies in your data pipelines.</p>"},{"location":"6-schema-validation/","title":"Schema validation and Data Contracts","text":"<p>Verbatim from the docs</p> <ul> <li>The schema describes the structure of normalized data (e.g., tables, columns, data types, etc.) and provides instructions on how the data should be processed and loaded.</li> <li>dlt generates schemas from the data during the normalization process. Users can affect this standard behavior by providing hints that change how tables, columns, and other metadata are generated and how the data is loaded.</li> <li>Such hints can be passed in the code, i.e., to the <code>dlt.resource</code> decorator or <code>pipeline.run</code> method. Schemas can also be exported and imported as files, which can be directly modified.</li> <li><code>dlt</code> associates a schema with a source and a table schema with a resource.</li> </ul> <p>In short, <code>dlt</code> will infer the schema but we can force it to explicit types</p> <ol> <li>Using  a <code>dict</code> specification</li> <li>Using a <code>Pydantic</code> model</li> <li>Using <code>hints</code></li> </ol>"},{"location":"6-schema-validation/#schema-definitions","title":"Schema definitions","text":"<p>Examples use the <code>replace</code> write disposition</p> <p>The examples in this section use the <code>replace</code> write disposition for simplicity. In a production scenario, you may want to use <code>append</code> or <code>merge</code> to preserve existing data.</p> <p>Given our previous example, we can define the schema for the <code>sample_data</code> resource using a <code>dict</code> specification:</p> <pre><code>    my_data = [\n        {\n            \"id\": 1,\n            \"name\": \"Mr. Mario\",\n            \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n            \"created_at\": \"2025-10-09 14:40:00\",\n            \"updated_at\": \"2025-10-09 14:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Mr. Luigi\",\n            \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n            \"created_at\": \"2025-10-08 16:15:00\",\n            \"updated_at\": \"2025-10-08 16:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n    ]\n</code></pre> <p>Using <code>dlt</code> we can define the schema as follows:</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition=\"replace\",\n    columns={\n        \"id\": {\"data_type\": \"bigint\"},\n        \"name\": {\"data_type\": \"text\"},\n        \"uuid\": {\"data_type\": \"text\"},\n        \"created_at\": {\"data_type\": \"timestamp\"},\n        \"updated_at\": {\"data_type\": \"timestamp\"},\n    },\n    nested_hints={\n        \"metadata\": dlt.mark.make_nested_hints(\n            columns=[\n                {\n                    \"name\": \"ingested_at\",\n                    \"data_type\": \"timestamp\",\n                },\n                {\n                    \"name\": \"script_name\",\n                    \"data_type\": \"text\",\n                },\n            ]\n        ),\n    },\n    schema_contract={\n        \"tables\": \"evolve\",\n        \"columns\": \"freeze\",\n        \"data_type\": \"evolve\",\n    },\n)\ndef sample_data() -&gt; TDataItems:\n</code></pre> <p>For now, ignore the <code>schema_contract</code> argument, as we will explain it later.</p> <p>For nested fields it gets a bit more complex. We can use the <code>nested_columns</code> argument to define the schema for nested fields:</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition=\"replace\",\n    columns={\n        \"id\": {\"data_type\": \"bigint\"},\n        \"name\": {\"data_type\": \"text\"},\n        \"uuid\": {\"data_type\": \"text\"},\n        \"created_at\": {\"data_type\": \"timestamp\"},\n        \"updated_at\": {\"data_type\": \"timestamp\"},\n    },\n    nested_hints={\n        \"metadata\": dlt.mark.make_nested_hints(\n            columns=[\n                {\n                    \"name\": \"ingested_at\",\n                    \"data_type\": \"timestamp\",\n                },\n                {\n                    \"name\": \"script_name\",\n                    \"data_type\": \"text\",\n                },\n            ]\n        ),\n    },\n    schema_contract={\n        \"tables\": \"evolve\",\n        \"columns\": \"freeze\",\n        \"data_type\": \"evolve\",\n    },\n)\ndef sample_data() -&gt; TDataItems:\n</code></pre>"},{"location":"6-schema-validation/#using-pydantic-models","title":"Using Pydantic models","text":"<p>Alternatively, we can use <code>Pydantic</code> models to define the schema:</p> <pre><code>class SampleDataMetadataModel(BaseModel):\n    ingested_at: dt.datetime\n    script_name: str\n\n\nclass SampleDataModel(BaseModel):\n    id: int\n    name: str\n    uuid: UUID\n    created_at: dt.datetime\n    updated_at: dt.datetime\n    metadata: SampleDataMetadataModel\n</code></pre> <p>And replace the <code>columns</code> argument in the <code>dlt.resource</code> decorator with the <code>Pydantic</code> model:</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition=\"replace\",\n    columns=SampleDataModel,\n    schema_contract={\n        \"tables\": \"evolve\",\n        \"columns\": \"freeze\",\n        \"data_type\": \"freeze\",\n    },\n)\ndef sample_data() -&gt; TDataItems:\n</code></pre>"},{"location":"6-schema-validation/#data-contracts","title":"Data contracts","text":"<p>Previously we mentioned the <code>schema_contract</code> argument in the <code>dlt.resource</code> decorator. This allows us to define a data contract that specifies how <code>dlt</code> should handle schema changes during data loading.</p> <p><code>dlt</code> will handle changes in tables, columns and data types by default. You can set its behaviour explicitly by passing values to the <code>schema_contract</code> argument of the <code>dlt.resource</code> decorator, such as:</p> <pre><code>@dlt.resource(\n schema_contract={\n        \"tables\": \"evolve\",\n        \"columns\": \"freeze\",\n        \"data_type\": \"freeze\",\n    })\ndef my_resource():\n  ...\n</code></pre> <p>You can control the following schema entities:</p> <ul> <li><code>tables</code> - the contract is applied when a new table is created</li> <li><code>columns</code> - the contract is applied when a new column is created on an existing table</li> <li><code>data_type</code> - the contract is applied when data cannot be coerced into a data type associated with an existing column.</li> </ul> <p>You can use contract modes to tell <code>dlt</code> how to apply the contract for a particular entity:</p> <ul> <li><code>evolve</code>: No constraints on schema changes.</li> <li><code>freeze</code>: This will raise an exception if data is encountered that does not fit the existing schema, so no data will be loaded to the destination.</li> <li><code>discard_row</code>: This will discard any extracted row if it does not adhere to the existing schema, and this row will not be loaded to the destination.</li> <li><code>discard_value</code>: This will discard data in an extracted row that does not adhere to the existing schema, and the row will be loaded without this data.</li> </ul> <p>How does \"evolve\" work?</p> <p>The default mode (evolve) works as follows:</p> <ol> <li>New tables may always be created.</li> <li>New columns may always be appended to the existing table.</li> <li>Data that do not coerce to the existing data type of a particular column will be sent to a variant column created for this particular type.</li> </ol>"},{"location":"6-schema-validation/#testing-contract-enforcement","title":"Testing contract enforcement","text":"<p>Consider the first schema definition using a <code>dict</code> specification, where we defined that neither columns nor data types should change:</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition=\"replace\",\n    columns={\n        \"id\": {\"data_type\": \"bigint\"},\n        \"name\": {\"data_type\": \"text\"},\n        \"uuid\": {\"data_type\": \"text\"},\n        \"created_at\": {\"data_type\": \"timestamp\"},\n        \"updated_at\": {\"data_type\": \"timestamp\"},\n    },\n    nested_hints={\n        \"metadata\": dlt.mark.make_nested_hints(\n            columns=[\n                {\n                    \"name\": \"ingested_at\",\n                    \"data_type\": \"timestamp\",\n                },\n                {\n                    \"name\": \"script_name\",\n                    \"data_type\": \"text\",\n                },\n            ]\n        ),\n    },\n    schema_contract={\n        \"tables\": \"evolve\",\n        \"columns\": \"freeze\",\n        \"data_type\": \"evolve\",\n    },\n)\ndef sample_data() -&gt; TDataItems:\n</code></pre> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition=\"replace\",\n    columns=SampleDataModel,\n    schema_contract={\n        \"tables\": \"evolve\",\n        \"columns\": \"freeze\",\n        \"data_type\": \"freeze\",\n    },\n)\ndef sample_data() -&gt; TDataItems:\n</code></pre> What happens if we set the <code>id=1</code> to <code>id='oops, this is a string now'</code> instead? <p>You should get an error like this:</p> <pre><code>&lt;class 'dlt.normalize.exceptions.NormalizeJobFailed'&gt;\nJob for `job_id=samples.e278217ef2.typed-jsonl.gz` failed terminally in load with `load_id=1762124329.823492` with message: In schema `sample_pipeline_postgres`: In Table: `samples` Column: `id__v_text` . Contract on `data_type` with `contract_mode=freeze` is violated. Can't add variant column `id__v_text` for table `samples` because `data_types` are frozen. Offending data item: id: None.\n</code></pre> <p>If we try the same with the Pydantic model schema definition, we will get a similar error:</p> <p>Check the documentation for more details</p> <p>See https://dlthub.com/docs/general-usage/schema-contracts for more information on data contracts and schema validation.</p>"},{"location":"7-next-steps/","title":"Conclusions and Next Steps","text":"<p>Hopefully by now you have a good understanding of how DLT works and how to use it to build data pipelines. Although the dlt documentation is quite extensive, is still a bit rough around the edges and can be hard to navigate at times. We tried to cover what I feel are the most important aspects of dlt to get you started, but there is much more to explore.</p> <p>Here are some suggestions for what to do next:</p>"},{"location":"7-next-steps/#ask-the-community-and-the-dlthub-bot-on-slack","title":"Ask the community and the <code>dlthub bot</code> on Slack","text":"<p>If you have questions or need help, you can join the DLT Slack community and ask your questions there. The community is very active and helpful.</p>"},{"location":"7-next-steps/#explore-the-dlt-cli","title":"Explore the <code>dlt</code> CLI","text":"<p>The <code>dlt</code> command-line interface (CLI) provides some commands to manage and run your pipelines. You can explore the available commands by running:</p> <pre><code>dlt --help\n</code></pre> <p>It may be of particular interest to explore the dashboard commands:</p> <pre><code>dlt pipeline sample_pipeline show\n</code></pre> <p>Refer to the official documentation for more details on the available commands and their usage.</p>"},{"location":"7-next-steps/#use-the-dlt-init-command-and-benefit-from-project-templates","title":"Use the <code>dlt init</code> command and benefit from project templates","text":"<p>Although in the official documentation this is introduced very early, I find it more useful once you have a better understanding of how dlt works. The <code>dlt init</code> command allows you to create a new dlt project from a template. You can explore the available templates by running:</p> <pre><code>dlt init &lt;SOURCE&gt; &lt;DESTINATION&gt;\n</code></pre> <p>For example, try:</p> <pre><code>mkdir my_dlt_project\ncd my_dlt_project\ndlt init postgres duckdb\n</code></pre> <p>This will bootstrap a new dlt project that extracts data from a Postgres database and loads it into a DuckDB database (And some other things too).</p> <p>Templates may be overwhelming at first</p> <p>The generated code may be overwhelming at first, but hopefully you understand now how the different parts work together.</p> <p>You can check the available templates using:</p> <pre><code>dlt init --list-sources\n</code></pre> <p>and</p> <pre><code>dlt init --list-destinations\n</code></pre>"},{"location":"7-next-steps/#explore-more-advanced-tutorials-and-courses","title":"Explore more advanced tutorials and courses","text":"<p>If you feel that <code>dlt</code> is a good fit for your data loading needs, you can explore more advanced tutorials and courses available on the official documentation site at https://dlthub.com/docs/tutorial/education:</p> <ul> <li>https://dlthub.com/docs/tutorial/fundamentals-course</li> <li>https://dlthub.com/docs/tutorial/advanced-course</li> </ul> <p>Until next time, happy data loading!</p>"}]}