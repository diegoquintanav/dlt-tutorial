{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenido","text":"<p>Este repositorio sirve como un tutorial pr\u00e1ctico para usar <code>dlt</code> (dlthub.com). Recopila lecciones paso a paso, c\u00f3digo de ejemplo y notas de configuraci\u00f3n dise\u00f1adas para ayudar a los lectores a comprender r\u00e1pidamente los conceptos b\u00e1sicos y reproducir los ejemplos localmente.</p> <p>Sigue la secci\u00f3n Primeros Pasos para configurar el entorno, luego avanza a trav\u00e9s de los tutoriales numerados y ejemplos en el directorio <code>dlt_tutorial/</code>.</p>"},{"location":"0-introduction/","title":"Introducci\u00f3n","text":"<p>Esta es la documentaci\u00f3n del tutorial de <code>dlt</code> (kit de herramientas de carga de datos). No confundas <code>dlt</code> con <code>Delta Live Tables</code> de Databricks. Esto no tiene nada que ver con ello.</p> <p>Usa <code>\"dlthub\"</code> como palabra clave para buscar contenido relacionado en motores de b\u00fasqueda</p> <p>De lo contrario, puedes encontrar contenido no relacionado sobre Delta Live Tables. Cuando decimos <code>dlt</code> en esta documentaci\u00f3n, siempre nos referimos al <code>kit de herramientas de carga de datos</code>.</p> <p>En pipelines de datos, un acr\u00f3nimo com\u00fanmente usado es ETL (Extraer, Transformar, Cargar).</p> <p>La extracci\u00f3n tiene que ver con obtener datos de un sistema fuente (por ejemplo, una API, una base de datos, archivos, etc.). La transformaci\u00f3n tiene que ver con limpiar, normalizar y dar forma a los datos para que se ajusten al sistema de destino. La carga tiene que ver con escribir los datos en el sistema de destino (por ejemplo, un almac\u00e9n de datos, un lago de datos, etc.).</p> <p>Alternativamente, un paradigma m\u00e1s reciente es ELT (Extraer, Cargar, Transformar). Esto invierte el orden de los dos \u00faltimos pasos, cargando primero los datos sin procesar en el sistema de destino, y luego transform\u00e1ndolos all\u00ed. Esto permite m\u00e1s flexibilidad y escalabilidad, cargando los datos primero y transform\u00e1ndolos m\u00e1s tarde seg\u00fan sea necesario. Esto puede ser especialmente \u00fatil cuando se trata de grandes vol\u00famenes de datos o cuando la l\u00f3gica de transformaci\u00f3n es compleja y puede cambiar con el tiempo.</p> <p><code>dlt</code> se enfoca en la parte de carga (<code>L</code>) de estos paradigmas. Nuestro requisito principal es mover datos de un sistema fuente a un sistema de destino, mientras otorgamos cierta flexibilidad sobre cu\u00e1ndo y c\u00f3mo transformar los datos. El c\u00f3digo para la parte de carga es mayormente repetitivo, para algunos escenarios comunes.</p> <p>La mayor\u00eda de las veces, la carga de datos no es una tarea \u00fanica. Los datos se generan continuamente en los sistemas fuente, y necesitamos mantener nuestros sistemas de destino actualizados con los datos m\u00e1s recientes. Aqu\u00ed es donde <code>dlt</code> brilla, proporcionando caracter\u00edsticas para carga incremental, captura de cambios de datos y programaci\u00f3n.</p>"},{"location":"0-introduction/#alternativas-a-dlt","title":"Alternativas a <code>dlt</code>","text":"<p>Hay muchas alternativas a <code>dlt</code> que abordan las tareas de carga de datos. La lista es enorme y no estoy listando todo, pero aqu\u00ed est\u00e1n las que he usado y/o considero relevantes:</p> <ul> <li>Apache NiFi: Una herramienta de integraci\u00f3n de datos de c\u00f3digo abierto que soporta enrutamiento de datos, transformaci\u00f3n y l\u00f3gica de mediaci\u00f3n de sistemas. Proporciona una interfaz basada en web para dise\u00f1ar flujos de datos y soporta una amplia gama de fuentes y destinos de datos. Ver https://nifi.apache.org/</li> <li>Airbyte: Una plataforma de integraci\u00f3n de datos de c\u00f3digo abierto que se enfoca en ELT. Proporciona una amplia gama de conectores para varias fuentes y destinos de datos, y permite a los usuarios definir l\u00f3gica de transformaci\u00f3n usando SQL. Ver https://airbyte.com/</li> <li>Meltano: Una plataforma de integraci\u00f3n de datos de c\u00f3digo abierto que se enfoca en ELT. Proporciona una amplia gama de conectores para varias fuentes y destinos de datos, y permite a los usuarios definir l\u00f3gica de transformaci\u00f3n usando archivos de configuraci\u00f3n yaml. Ver https://meltano.com/</li> <li>Dagster: Un orquestador de datos de c\u00f3digo abierto para aprendizaje autom\u00e1tico, anal\u00edtica y ETL. Proporciona un marco para construir, programar y monitorear pipelines de datos. Ver https://dagster.io/.</li> </ul>"},{"location":"0-introduction/#por-que-dlt","title":"\u00bfPor qu\u00e9 <code>dlt</code>?","text":""},{"location":"0-introduction/#menos-es-mas","title":"Menos es m\u00e1s","text":"<p>O como dice en el Zen de python: \"Simple es mejor que complejo.\"</p> <p>La mayor\u00eda de las alternativas mencionadas anteriormente son herramientas poderosas y flexibles para tareas de carga de datos. Sin embargo, tambi\u00e9n pueden ser complejas y requerir configuraci\u00f3n significativa.</p> <p>Por ejemplo, Meltano es muy flexible a trav\u00e9s de sus archivos de configuraci\u00f3n YAML, pero esto tambi\u00e9n significa que necesitas aprender su sintaxis y estructura de configuraci\u00f3n. Para Dagster necesitas aprender su marco y API, y hospedar el servicio en alg\u00fan lugar. Para DLT, necesitas Python en su mayor parte.</p>"},{"location":"0-introduction/#tareas-puntuales","title":"Tareas puntuales","text":"<p>Un pipeline de <code>dlt</code> puede ser tan simple como un script de Python que puedes ejecutar desde tu m\u00e1quina local o un servidor. Esto hace que sea f\u00e1cil de configurar y usar para tareas de carga de datos puntuales, sin la necesidad de infraestructura compleja o configuraci\u00f3n.</p> <p>Una vez que superas la curva de aprendizaje inicial, es f\u00e1cil de usar.</p>"},{"location":"0-introduction/#ambientalmente-amigable","title":"Ambientalmente amigable","text":"<p>Dependiendo del tama\u00f1o de tus datos y la frecuencia de tus cargas, usar una herramienta ligera como <code>dlt</code> puede ser m\u00e1s amigable con el medio ambiente que usar una plataforma de integraci\u00f3n de datos pesada que requiere recursos computacionales significativos.</p> <p>Mide tu huella de carbono</p> <p>Ver https://codecarbon.io/ si deseas comenzar a medir las emisiones de carbono de tu c\u00f3digo.</p>"},{"location":"0-introduction/#codigo-abierto","title":"C\u00f3digo abierto","text":"<p>El c\u00f3digo tiene una licencia Apache 2.0, por lo que puedes usarlo libremente en tus proyectos, incluso comerciales. Tambi\u00e9n puedes contribuir al proyecto si deseas mejorarlo o agregar nuevas caracter\u00edsticas.</p> <p>Tiene una versi\u00f3n <code>dlt+</code> que no he explorado ni necesitado a\u00fan, que parece agregar m\u00e1s caracter\u00edsticas y soporte.</p>"},{"location":"0-introduction/#contras-de-dlt","title":"Contras de <code>dlt</code>","text":"<p>Siento que podr\u00eda usar m\u00e1s amor en t\u00e9rminos de</p> <ul> <li>Caracter\u00edsticas de CLI: la CLI es torpe y a veces se siente in\u00fatil o tosca</li> <li>Dashboards: <code>dlt</code> tiene algunos elementos internos que usa para mantener y rastrear el estado. Tambi\u00e9n proporciona dashboards que ofrecen una perspectiva sobre estos datos. No he usado ni necesitado estos dashboards pero puedo entender que algunas personas los quieren y los usan.</li> </ul>"},{"location":"0-introduction/#el-estilo-en-la-documentacion-no-es-consistente","title":"El estilo en la documentaci\u00f3n no es consistente","text":"<p>Escribir documentaci\u00f3n es dif\u00edcil. Yo mismo lucho con ello. Uno de los versos del Zen de Python afirma:</p> <p>Deber\u00eda haber una-- y preferiblemente solo una --manera obvia de hacerlo.</p> <p>Los documentos muestran m\u00faltiples formas de lograr lo mismo, lo que puede ser confuso para los usuarios. A veces faltan importaciones, algunas explicaciones sobre el uso no est\u00e1n claras, y hay inconsistencias en los ejemplos proporcionados. Se siente ca\u00f3tico a veces.</p>"},{"location":"1-getting-started/","title":"Primeros Pasos con este Tutorial","text":"<p>Asumo que usas Linux</p> <p>Este tutorial asume que est\u00e1s usando un sistema operativo basado en Linux. Aunque puede ser posible seguir adelante en otros sistemas operativos, algunos comandos e instrucciones pueden diferir.</p>"},{"location":"1-getting-started/#usando-docker-y-docker-compose","title":"Usando Docker y Docker Compose","text":"<p>Necesitar\u00e1s docker, docker-compose, python y <code>uv</code> instalados en tu sistema para seguir este tutorial.</p> <ol> <li>Instala docker desde https://docs.docker.com/get-docker/</li> <li>Instala docker-compose desde https://docs.docker.com/compose/install/</li> </ol>"},{"location":"1-getting-started/#que-pasa-si-no-quiero-usar-docker","title":"\u00bfQu\u00e9 pasa si no quiero usar Docker?","text":"<p>Necesitar\u00e1s configurar tu entorno local para ejecutar este tutorial. Las siguientes secciones te guiar\u00e1n a trav\u00e9s de los pasos necesarios.</p> <ol> <li>Instala python usando tu m\u00e9todo preferido. Aqu\u00ed hay algunas opciones:<ul> <li>Desde el sitio web oficial: https://www.python.org/downloads/</li> <li>Usando un gestor de paquetes como <code>apt</code>, como <code>apt install python3</code> en Debian/Ubuntu</li> <li>Usando un gestor de versiones como <code>pyenv</code>: https://github.com/pyenv/pyenv</li> <li>Usando un gestor de versiones como <code>uv</code>: https://uv.io/docs/getting-started/installation/</li> </ul> </li> <li>Instala <code>uv</code> siguiendo las instrucciones en https://uv.io/docs/getting-started/installation/</li> </ol>"},{"location":"1-getting-started/#usando-python-puro","title":"Usando Python puro","text":"<p>Este tutorial fue escrito usando <code>Python 3.12.3</code>. Puedes crear un entorno virtual usando <code>venv</code> o <code>virtualenv</code> para aislar las dependencias de este tutorial.</p> <p>Si est\u00e1s usando <code>uv</code>, puedes crear un entorno virtual usando <code>uv venv</code>:</p> <pre><code>uv venv --seed\nuv sync\nsource .venv/bin/activate\n</code></pre> <p>Si est\u00e1s usando Python puro, puedes crear un entorno virtual usando <code>venv</code> de la librer\u00eda est\u00e1ndar:</p> <pre><code>python3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n</code></pre> <p>requirements.txt</p> <p>El archivo <code>requirements.txt</code> fue generado usando <code>uv export --format requirements.txt -o requirements.txt</code> para asegurar compatibilidad con <code>uv</code>.</p>"},{"location":"1-getting-started/#instalando-duckdb","title":"Instalando duckdb","text":"<p>Sigue las instrucciones en https://duckdb.org/install/?platform=linux&amp;environment=cli para instalar el cliente <code>duckdb</code> en tu sistema.</p>"},{"location":"1-getting-started/#configurando-una-base-de-datos-postgres","title":"Configurando una base de datos Postgres","text":"<p>Se requiere una base de datos Postgres para ejecutar este tutorial. Si tienes docker compose instalado, puedes configurar una base de datos Postgres local usando Docker y Docker Compose.</p> <pre><code>docker compose -f docker-compose.yml up db -d\n</code></pre> <p>Esto iniciar\u00e1 una base de datos Postgres en <code>localhost:5555</code> con las siguientes credenciales (todos los valores por defecto de la documentaci\u00f3n oficial, pero la contrase\u00f1a ha sido cambiada a <code>test</code>):</p> <ul> <li>Usuario: <code>postgres</code></li> <li>Contrase\u00f1a: <code>test</code></li> <li>Base de datos: <code>postgres</code></li> <li>Host: <code>localhost</code></li> <li>Puerto: <code>5555</code></li> </ul> <p>Puedes conectarte a esta base de datos usando cualquier cliente de Postgres, como <code>psql</code> o una herramienta GUI como DBeaver o pgAdmin. Se proporciona una conexi\u00f3n de acceso r\u00e1pido a trav\u00e9s del <code>Makefile</code></p> <pre><code>make postgres.psql\n</code></pre>"},{"location":"1-getting-started/#configurando-una-base-de-datos-postgres-remota-usando-neon","title":"Configurando una base de datos Postgres remota usando Neon","text":"<p>Puedes configurar una base de datos Postgres externa si lo prefieres.</p> <p>Una instancia postgres gratuita puede ser creada usando servicios como https://neon.com/ (no estoy afiliado con ellos, solo parece menos complicado).</p> <p>Crea una cuenta y obt\u00e9n tu cadena de conexi\u00f3n, y deber\u00eda verse algo as\u00ed:</p> <pre><code>psql 'postgresql://neondb_owner:npg_some_long_url.aws.neon.tech/neondb?sslmode=require&amp;channel_binding=require'\n</code></pre> <p>Aseg\u00farate de que cuando generes la cadena de conexi\u00f3n, desactives el pooling de conexiones si esa opci\u00f3n est\u00e1 habilitada.</p> <p></p> <p>Necesitar\u00e1s esta cadena de conexi\u00f3n, y veremos c\u00f3mo usarla en las siguientes secciones.</p>"},{"location":"1-getting-started/#usando-devcontainers","title":"Usando devcontainers","text":"<p>Se proporciona un devcontainer para este tutorial. Si est\u00e1s usando VSCode y tienes docker instalado, puedes abrir esta carpeta en un devcontainer y tener todo configurado para ti. Ver https://code.visualstudio.com/docs/devcontainers/containers para m\u00e1s informaci\u00f3n sobre devcontainers.</p>"},{"location":"1-getting-started/#configuracion-de-conexion-a-base-de-datos-postgres-desde-dentro-del-devcontainer","title":"Configuraci\u00f3n de conexi\u00f3n a base de datos Postgres desde dentro del devcontainer","text":"<p>Si est\u00e1s usando el devcontainer, la base de datos Postgres puede ser accedida en <code>postgres:5432</code> con las mismas credenciales mencionadas anteriormente.</p>"},{"location":"2-our-first-pipeline/","title":"Un ejemplo simple usando <code>duckdb</code>","text":"<p>En su forma m\u00e1s b\u00e1sica, <code>dlt</code> toma un iterable que produce diccionarios. Tratar\u00e1 cada diccionario como una unidad de datos y lo mover\u00e1 a un <code>destino</code>.</p> <p>Toma por ejemplo el siguiente diccionario:</p> <pre><code>my_data = [\n    {\n        \"id\": 1,\n        \"name\": \"Mr. Mario\",\n        \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n        \"created_at\": \"2025-10-09 14:40:00\",\n        \"updated_at\": \"2025-10-09 14:50:00\",\n        \"metadata\": {\n            \"ingested_at\": dt.datetime.now().isoformat(),\n            \"script_name\": Path(__file__).name,\n        },\n    },\n    {\n        \"id\": 2,\n        \"name\": \"Mr. Luigi\",\n        \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n        \"created_at\": \"2025-10-08 16:15:00\",\n        \"updated_at\": \"2025-10-08 16:50:00\",\n        \"metadata\": {\n            \"ingested_at\": dt.datetime.now().isoformat(),\n            \"script_name\": Path(__file__).name,\n        },\n    },\n]\n</code></pre> <p>Aqu\u00ed hay un ejemplo simple que usa <code>dlt</code> para cargar datos en una base de datos <code>duckdb</code>.</p> <pre><code>    pipeline = dlt.pipeline(\n        pipeline_name=\"sample_pipeline\",\n        destination=dlt.destinations.duckdb,\n        dataset_name=\"sample_data\",\n    )\n    print(\"Running pipeline...\")\n\n    load_info = pipeline.run(\n        my_data,\n        table_name=\"samples\",\n        refresh=\"drop_sources\",\n        write_disposition={\n            \"disposition\": \"replace\",\n        },\n    )\n</code></pre> <p>dlt no se preocupa por c\u00f3mo produces los datos</p> <p>La fuente de datos puede ser cualquier cosa que produzca diccionarios. Por ejemplo, puedes usar <code>pandas</code>, ej. <code>[row.to_dict() for _, row in df.iterrows()]</code>.</p>"},{"location":"2-our-first-pipeline/#que-esta-pasando-aqui","title":"\u00bfQu\u00e9 est\u00e1 pasando aqu\u00ed?","text":"<ol> <li>Definimos una lista de diccionarios llamada <code>my_data</code>. Cada diccionario representa un registro con campos como <code>id</code>, <code>name</code>, <code>age</code> y <code>address</code>.</li> <li>Creamos un pipeline de <code>dlt</code> usando <code>dlt.pipeline()</code>, especificando el nombre del pipeline y el destino como <code>duckdb</code>.</li> <li>Usamos <code>pipeline.run(my_data)</code> para cargar los datos de <code>my_data</code> en la base de datos <code>duckdb</code>. El par\u00e1metro <code>write_disposition</code> est\u00e1 configurado como <code>replace</code>, lo que significa que si la tabla de destino ya existe, ser\u00e1 reemplazada con los nuevos datos. Esto es \u00fatil en este ejemplo para asegurar que comenzamos con una pizarra limpia cada vez que ejecutamos el script.</li> </ol>"},{"location":"2-our-first-pipeline/#ejecutando-el-ejemplo","title":"Ejecutando el ejemplo","text":"<p>Ejecuta el script directamente usando Python:</p> <pre><code>$ python dlt_tutorial/0_sample_pipeline_basic.py\nRunning pipeline...\nDone\n</code></pre> <p>Limitaciones de DuckDB en concurrencia</p> <p>Ten en cuenta que duckdb no permite acceso concurrente. Si intentas ejecutar el pipeline mientras tienes un cliente ya conectado a la base de datos, el pipeline fallar\u00e1.</p> <p>Esto crear\u00e1 un archivo de base de datos <code>duckdb</code> en el directorio actual (por defecto llamado <code>sample_pipeline.duckdb</code>) y cargar\u00e1 los datos en \u00e9l. Luego puedes consultar la base de datos usando cualquier cliente o librer\u00eda de <code>duckdb</code>.</p> <pre><code>$ duckdb sample_pipeline.duckdb -c \"select * from sample_data.samples;\"\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  id   \u2502   name    \u2502                 uuid                 \u2502        created_at        \u2502        updated_at        \u2502     metadata__ingested_at     \u2502   metadata__script_name    \u2502    _dlt_load_id    \u2502    _dlt_id     \u2502\n\u2502 int64 \u2502  varchar  \u2502               varchar                \u2502 timestamp with time zone \u2502 timestamp with time zone \u2502   timestamp with time zone    \u2502          varchar           \u2502      varchar       \u2502    varchar     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     1 \u2502 Mr. Mario \u2502 a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc \u2502 2025-10-09 11:40:00-03   \u2502 2025-10-09 11:50:00-03   \u2502 2025-10-29 14:32:44.702763-03 \u2502 0_sample_pipeline_basic.py \u2502 1761769965.1842644 \u2502 sVJ0ooLEPZsP1Q \u2502\n\u2502     2 \u2502 Mr. Luigi \u2502 8c804ede-f8ae-409e-964d-9e355a3094e0 \u2502 2025-10-08 13:15:00-03   \u2502 2025-10-08 13:50:00-03   \u2502 2025-10-29 14:32:44.702794-03 \u2502 0_sample_pipeline_basic.py \u2502 1761769965.1842644 \u2502 P7QNYrq+IMHy1A \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"2-our-first-pipeline/#que-esta-haciendo-dlt-internamente","title":"\u00bfQu\u00e9 est\u00e1 haciendo <code>dlt</code> internamente?","text":"<p>\u00a1Felicitaciones! Has cargado exitosamente datos en una base de datos <code>duckdb</code> usando <code>dlt</code>. Hay algunas cosas que est\u00e1n sucediendo internamente que vale la pena mencionar:</p> <ol> <li> <p>Cre\u00f3 una <code>tabla</code> llamada <code>sample_data.samples</code>. En esta tabla, cada fila tiene un <code>_dlt_load_id</code> y un <code>_dlt_id</code>. Estas son referencias a mecanismos internos de <code>dlt</code> para rastrear cargas e identificar registros.</p> </li> <li> <p><code>dlt</code> tambi\u00e9n crea tres tablas m\u00e1s que persisten el estado del pipeline</p> <ol> <li><code>_dlt_loads</code></li> <li><code>_dlt_pipeline_state</code></li> <li><code>_dlt_version</code></li> </ol> </li> <li> <p>Sin que lo sepas, <code>dlt</code> tambi\u00e9n cre\u00f3 un directorio en <code>~/.dlt/pipelines/sample_pipeline</code></p> </li> </ol>"},{"location":"2-our-first-pipeline/#resumiendo","title":"Resumiendo","text":"<p>Vimos un ejemplo simple de usar <code>dlt</code> para cargar datos en una base de datos <code>duckdb</code>.</p> <p>Subiremos el nivel en la siguiente parte usando <code>resources</code> y <code>sources</code> para estructurar mejor nuestra extracci\u00f3n de datos.</p>"},{"location":"2-our-first-pipeline/#ejemplo-completo","title":"Ejemplo completo","text":"<pre><code>import datetime as dt\nfrom pathlib import Path\nimport dlt\n\nmy_data = [\n    {\n        \"id\": 1,\n        \"name\": \"Mr. Mario\",\n        \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n        \"created_at\": \"2025-10-09 14:40:00\",\n        \"updated_at\": \"2025-10-09 14:50:00\",\n        \"metadata\": {\n            \"ingested_at\": dt.datetime.now().isoformat(),\n            \"script_name\": Path(__file__).name,\n        },\n    },\n    {\n        \"id\": 2,\n        \"name\": \"Mr. Luigi\",\n        \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n        \"created_at\": \"2025-10-08 16:15:00\",\n        \"updated_at\": \"2025-10-08 16:50:00\",\n        \"metadata\": {\n            \"ingested_at\": dt.datetime.now().isoformat(),\n            \"script_name\": Path(__file__).name,\n        },\n    },\n]\n\n\nif __name__ == \"__main__\":\n    pipeline = dlt.pipeline(\n        pipeline_name=\"sample_pipeline\",\n        destination=dlt.destinations.duckdb,\n        dataset_name=\"sample_data\",\n    )\n    print(\"Running pipeline...\")\n\n    load_info = pipeline.run(\n        my_data,\n        table_name=\"samples\",\n        refresh=\"drop_sources\",\n        write_disposition={\n            \"disposition\": \"replace\",\n        },\n    )\n\n    print(\"Done\")\n</code></pre>"},{"location":"3-sources-and-resources/","title":"Parte 1: Usando <code>resources</code> y <code>sources</code> para estructurar la extracci\u00f3n de datos","text":"<p>En la parte anterior, vimos un ejemplo simple de usar <code>dlt</code> para cargar datos en una base de datos <code>duckdb</code>. Aunque ese ejemplo fue directo, podemos hacerlo mejor.</p>"},{"location":"3-sources-and-resources/#una-introduccion-a-iteradores-y-generadores","title":"Una introducci\u00f3n a iteradores y generadores","text":"<p>Un concepto importante en Python que <code>dlt</code> aprovecha es la idea de iteradores y generadores. Estas construcciones permiten un bucle eficiente sobre datos sin la necesidad de cargar todo en memoria de una vez.</p> <ul> <li>Un iterador es un objeto que implementa el protocolo iterador, que consiste en los m\u00e9todos <code>__iter__()</code> y <code>__next__()</code>. Un iterador te permite recorrer todos los elementos de una colecci\u00f3n, como una lista o un conjunto, uno a la vez.</li> <li>Un generador es un tipo especial de iterador que se define usando una funci\u00f3n y la palabra clave <code>yield</code>. Los generadores te permiten producir una secuencia de valores a lo largo del tiempo, en lugar de calcularlos todos a la vez y almacenarlos en memoria.</li> </ul> <p>Aqu\u00ed hay un ejemplo simple de una funci\u00f3n generadora que produce n\u00fameros del 0 al n-1:</p> <pre><code>def count_up_to(n):\n    count = 0\n    while count &lt; n:\n        yield count\n        count += 1\n</code></pre> <p>Puedes usar este generador as\u00ed:</p> <pre><code>for number in count_up_to(5):\n    print(number)\n</code></pre> <p>Esto producir\u00e1:</p> <pre><code>0\n1\n2\n3\n4\n</code></pre> <p>Los generadores son particularmente \u00fatiles cuando se trata de grandes conjuntos de datos o flujos de datos, ya que te permiten procesar un elemento a la vez sin cargar todo en memoria.</p> <p>Sin embargo, los generadores solo pueden ser iterados una vez. Despu\u00e9s de que se agotan, no pueden ser reutilizados. Si necesitas iterar sobre los datos m\u00faltiples veces, necesitar\u00e1s crear una nueva instancia del generador cada vez.</p> <pre><code>&gt;&gt;&gt; my_gen_5 = count_up_to(5)\n\n&gt;&gt;&gt; my_gen_5\n&lt;generator object count_up_to at 0x71aa29e4bb80&gt;\n\n&gt;&gt;&gt; [num for num in my_gen_5]\n[0, 1, 2, 3, 4]\n\n&gt;&gt;&gt; [num for num in my_gen_5]\n[]\n</code></pre>"},{"location":"3-sources-and-resources/#reescribiendo-nuestro-ejemplo-anterior-usando-un-generador","title":"Reescribiendo nuestro ejemplo anterior usando un generador","text":"<p>Reescribamos nuestro ejemplo anterior usando una funci\u00f3n generadora para producir los datos. Esto nos permitir\u00e1 manejar conjuntos de datos m\u00e1s grandes de manera m\u00e1s eficiente.</p> <pre><code>def sample_data() -&gt; Generator[dict, None, None]:\n    data = [\n        {\n            \"id\": 1,\n            \"name\": \"Mr. Mario\",\n            \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n            \"created_at\": \"2025-10-09 14:40:00\",\n            \"updated_at\": \"2025-10-09 14:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Mr. Luigi\",\n            \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n            \"created_at\": \"2025-10-08 16:15:00\",\n            \"updated_at\": \"2025-10-08 16:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n    ]\n    for item in data:\n        yield item\n</code></pre> <p>Ahora podemos pasar esta funci\u00f3n generadora a nuestro pipeline de <code>dlt</code>:</p> <pre><code>    pipeline = dlt.pipeline(\n        pipeline_name=\"sample_pipeline\",\n        destination=dlt.destinations.duckdb,\n        dataset_name=\"sample_data\",\n    )\n\n    print(\"Running pipeline...\")\n\n    load_info = pipeline.run(\n        sample_data,\n        table_name=\"samples\",\n        refresh=\"drop_sources\",\n        write_disposition={\n            \"disposition\": \"replace\",\n        },\n    )\n</code></pre> <p>Esta vez, sin embargo, estamos imprimiendo el resultado del m\u00e9todo <code>pipeline.run()</code>, que devuelve un objeto <code>LoadInfo</code> que contiene detalles sobre el proceso de carga.</p> <pre><code>    print(\"Load info:\")\n    print(load_info)\n</code></pre> <p>Puedes ejecutar este script de la misma manera que antes</p> <pre><code>$ python dlt_tutorial/1_sample_pipeline_basic.py\nRunning pipeline...\nDone\nLoad info:\nPipeline sample_pipeline load step completed in 0.28 seconds\n1 load package(s) were loaded to destination duckdb and into dataset sample_data\nThe duckdb destination used duckdb:////home/diego/Code/playground/dlt-tutorial/sample_pipeline.duckdb location to store data\nLoad package 1762003350.048731 is LOADED and contains no failed jobs\n</code></pre> <p>\u00bfQu\u00e9 es la informaci\u00f3n de carga?</p> <p>La informaci\u00f3n de carga proporciona detalles sobre el proceso de carga, incluyendo el tiempo tomado, el n\u00famero de paquetes de carga y el destino donde se almacenaron los datos. Tambi\u00e9n indica si hubo trabajos fallidos durante el proceso de carga.</p> <p>Usando generadores con dlt</p> <p><code>dlt</code> funciona mejor si el generador produce diccionarios en lotes, ver https://dlthub.com/docs/reference/performance#yield-pages-instead-of-rows</p>"},{"location":"3-sources-and-resources/#usando-resources-y-sources","title":"Usando <code>resources</code> y <code>sources</code>","text":"<ul> <li>Un resource es una funci\u00f3n (opcionalmente async) que produce datos. Para crear un resource, agregamos el decorador <code>@dlt.resource</code> a esa funci\u00f3n.</li> <li>Un source es una funci\u00f3n decorada con <code>@dlt.source</code> que devuelve uno o m\u00e1s resources.</li> </ul> <p>En su implementaci\u00f3n m\u00e1s b\u00e1sica, un <code>resource</code> se implementa decorando nuestra funci\u00f3n generadora de datos con <code>@dlt.resource</code>:</p> <pre><code>@dlt.resource\ndef sample_data() -&gt; Generator[dict, None, None]:\n    my_data = [\n        {\n            \"id\": 1,\n            \"name\": \"Mr. Mario\",\n            \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n            \"created_at\": \"2025-10-09 14:40:00\",\n            \"updated_at\": \"2025-10-09 14:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Mr. Luigi\",\n            \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n            \"created_at\": \"2025-10-08 16:15:00\",\n            \"updated_at\": \"2025-10-08 16:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n    ]\n    for item in my_data:\n        yield item\n</code></pre> <p>Un <code>source</code> se crea definiendo una funci\u00f3n que devuelve el resource:</p> <pre><code>@dlt.source\ndef sample_source():\n    yield sample_data\n</code></pre> <p>Luego podemos usar este source en nuestro pipeline:</p> <pre><code>    pipeline = dlt.pipeline(\n        pipeline_name=\"sample_pipeline\",\n        destination=dlt.destinations.duckdb,\n        dataset_name=\"sample_data\",\n    )\n\n    load_info = pipeline.run(\n        sample_source,\n        table_name=\"samples\",\n        refresh=\"drop_sources\",\n        write_disposition={\n            \"disposition\": \"replace\",\n        },\n    )\n\n    print(load_info)\n</code></pre>"},{"location":"3-sources-and-resources/#que-cambio","title":"\u00bfQu\u00e9 cambi\u00f3?","text":"<p><code>dlt</code> genera autom\u00e1ticamente especificaciones de configuraci\u00f3n para funciones decoradas con <code>@dlt.source</code>, <code>@dlt.resource</code> y <code>@dlt.destination</code></p> <p>Por ejemplo, ahora permite inyectar valores de configuraci\u00f3n</p>"},{"location":"3-sources-and-resources/#inyectando-valores-de-configuracion","title":"Inyectando valores de configuraci\u00f3n","text":"<p>Podemos modificar nuestra funci\u00f3n source para aceptar un par\u00e1metro de configuraci\u00f3n:</p> <pre><code>@dlt.source\ndef sample_source(my_custom_parameter: str = \"foo\"):\n    print(f\"Custom parameter value: {my_custom_parameter}\")\n    yield sample_data\n</code></pre> <p>Este par\u00e1metro ahora puede ser configurado usando variables de entorno, archivos de configuraci\u00f3n o argumentos de l\u00ednea de comandos cuando se ejecuta el pipeline.</p> <pre><code>$ MY_CUSTOM_PARAMETER=\"pythonchile\" python dlt_tutorial/2b_sample_pipeline_sources_resources_with_config.py\nCustom parameter value: pythonchile\nRunning pipeline...\nDone\n...\n</code></pre> <p>Alternativamente, podemos configurar el par\u00e1metro en un archivo de configuraci\u00f3n ubicado en <code>.dlt/config.toml</code>:</p> <pre><code>[sample_pipeline]\nmy_custom_parameter = \"baz\"\n</code></pre> <p>Ejecutar el pipeline ahora usar\u00e1 el valor del archivo de configuraci\u00f3n:</p> <pre><code>$ python dlt_tutorial/2b_sample_pipeline_sources_resources_with_config.py\nCustom parameter value: baz\nRunning pipeline...\nDone\n...\n</code></pre> <p>dlt busca valores en m\u00faltiples lugares en un orden espec\u00edfico</p> <p>Ver https://dlthub.com/docs/general-usage/credentials/setup#how-dlt-looks-for-values para m\u00e1s informaci\u00f3n sobre c\u00f3mo <code>dlt</code> busca valores de configuraci\u00f3n.</p> <p>Usaremos <code>resource</code> directamente de ahora en adelante</p> <p>De ahora en adelante, usaremos <code>resource</code> directamente en lugar de envolverlos en sources, a menos que necesitemos agrupar m\u00faltiples resources juntos.</p> <p>Dado que solo estamos usando un solo resource, podemos pasarlo directamente al m\u00e9todo <code>pipeline.run()</code>.</p>"},{"location":"3-sources-and-resources/#resumiendo","title":"Resumiendo","text":"<p>En esta parte, aprendimos sobre iteradores y generadores en Python, y c\u00f3mo pueden ser usados para manejar datos eficientemente en pipelines de <code>dlt</code>. Tambi\u00e9n introdujimos los conceptos de <code>resources</code> y <code>sources</code>, que ayudan a estructurar la extracci\u00f3n de datos de una manera m\u00e1s modular.</p> <p>En la siguiente parte, cambiaremos a un destino postgres y veremos c\u00f3mo manejar patrones de carga m\u00e1s complejos.</p>"},{"location":"4-switching-to-postgres/","title":"Parte 2: Implementando carga incremental de datos","text":""},{"location":"4-switching-to-postgres/#introduccion-a-la-carga-incremental","title":"Introducci\u00f3n a la carga incremental","text":"<p>Considera el siguiente estado de tu tabla en un momento dado:</p> id name created_at updated_at 1 Mr. Mario 2025-10-09 14:40:00 2025-10-09 14:50:00 2 Mr. Luigi 2025-10-08 16:15:00 2025-10-08 16:50:00 <p>En esta tabla, podemos identificar dos registros, que pueden ser identificados \u00fanicamente por sus campos <code>id</code> o <code>uuid</code>. Cada registro tambi\u00e9n tiene marcas de tiempo <code>created_at</code> y <code>updated_at</code>.</p> <p>Ahora, imagina que ha pasado algo de tiempo, y tenemos nuevos datos para cargar en nuestra tabla. Obtenemos nuestros datos de una API remota que sirve actualizaciones incrementales sobre informaci\u00f3n de Mario y Luigi.</p> <p>Los nuevos datos entrantes se ven as\u00ed:</p> id name created_at updated_at 1 Jumpman 2025-10-09 14:40:00 2025-10-10 11:50:00 3 Ms. Peach 2025-10-12 13:15:00 2025-10-13 13:50:00 <p>Mario ahora es conocido como \"Jumpman\", fue actualizado despu\u00e9s de su \u00faltima actualizaci\u00f3n, y un nuevo registro para \"Ms. Peach\" ha sido agregado.</p> <p>\u00bfCu\u00e1l deber\u00eda ser el estado final de nuestra tabla despu\u00e9s de cargar estos nuevos datos? Depende de la l\u00f3gica que queramos implementar para manejar actualizaciones e inserciones.</p> <p>Aunque hay muchas estrategias, aqu\u00ed hay algunas comunes:</p>"},{"location":"4-switching-to-postgres/#reemplazar-todo","title":"Reemplazar todo","text":"<p>Ignorar el estado existente y reemplazar todo: En este caso, simplemente reemplazar\u00edamos toda la tabla con los datos entrantes.</p> id name created_at updated_at 1 Jumpman 2025-10-09 11:40:00-03 2025-10-10 11:50:00-03 3 Ms. Peach 2025-10-10 13:25:00-03 2025-10-10 13:50:00-03"},{"location":"4-switching-to-postgres/#agregar-nuevos-datos","title":"Agregar nuevos datos","text":"<p>Simplemente agregar los datos entrantes a la tabla existente:</p> id name created_at updated_at 1 Mr. Mario 2025-10-09 11:40:00-03 2025-10-09 11:50:00-03 2 Mr. Luigi 2025-10-08 13:15:00-03 2025-10-08 13:50:00-03 1 Jumpman 2025-10-09 11:40:00-03 2025-10-10 11:50:00-03 3 Ms. Peach 2025-10-10 13:25:00-03 2025-10-10 13:50:00-03"},{"location":"4-switching-to-postgres/#fusionar-nuevos-registros-en-los-existentes","title":"Fusionar nuevos registros en los existentes","text":"<p>Podemos actualizar registros existentes o agregar nuevos. Para esto, necesitamos depender de alg\u00fan identificador como el <code>id</code>, que identifique \u00fanicamente un registro:</p> <p>Si consideramos el <code>id</code> como tal registro, actualizar\u00edamos el registro de Mario con el nuevo nombre y marca de tiempo actualizada, e insertar\u00edamos el nuevo registro para \"Ms. Peach\".</p> id name created_at updated_at 1 Jumpman 2025-10-09 11:40:00-03 2025-10-10 11:50:00-03 2 Mr. Luigi 2025-10-08 13:15:00-03 2025-10-08 13:50:00-03 3 Ms. Peach 2025-10-10 13:25:00-03 2025-10-10 13:50:00-03 <p>La estrategia de fusi\u00f3n es la m\u00e1s com\u00fan cuando se trata de carga incremental de datos. Nos permite mantener nuestros datos actualizados mientras minimizamos la cantidad de datos que necesitamos procesar.</p>"},{"location":"4-switching-to-postgres/#un-ejemplo-con-postgres","title":"Un ejemplo con Postgres","text":"<p>Un buen ejemplo de una base de datos que soporta carga incremental es Postgres. Postgres proporciona varios mecanismos para manejar actualizaciones e inserciones, como <code>UPSERT</code> (usando <code>ON CONFLICT</code>), o <code>MERGE</code> en versiones m\u00e1s recientes, que nos permite insertar nuevos registros o actualizar los existentes basados en una restricci\u00f3n \u00fanica.</p> <p>Seg\u00fan su documentaci\u00f3n, <code>MERGE</code> implementa una operaci\u00f3n de uni\u00f3n usando una <code>join_condition</code>.</p> <pre><code>[ WITH with_query [, ...] ]\nMERGE INTO [ ONLY ] target_table_name [ * ] [ [ AS ] target_alias ]\n    USING data_source ON join_condition\n    when_clause [...]\n    [ RETURNING [ WITH ( { OLD | NEW } AS output_alias [, ...] ) ]\n                { * | output_expression [ [ AS ] output_name ] } [, ...] ]\n</code></pre> <p>En el siguiente ejemplo, <code>MERGE</code> requiere una tabla <code>target</code> (la tabla que queremos actualizar) y una tabla <code>source</code> (los nuevos datos que queremos insertar o usar para actualizar el objetivo).</p> <pre><code>MERGE INTO target_table tt\n  USING source_table st\n    ON st.id = tt.id\nWHEN MATCHED THEN\n  UPDATE SET name = st.name\nWHEN NOT MATCHED THEN\n  INSERT (id, name)\n  VALUES (st.id, st.name);\n</code></pre> <p>En este ejemplo, estamos fusionando datos de <code>source_table</code> en <code>target_table</code> bas\u00e1ndose en la columna <code>id</code>. Si se encuentra un registro coincidente, actualizamos la columna <code>name</code>; si no, insertamos un nuevo registro.</p>"},{"location":"4-switching-to-postgres/#implementando-carga-incremental-con-dlt","title":"Implementando carga incremental con <code>dlt</code>","text":"<p>\u00bfC\u00f3mo sabemos qu\u00e9 escenario necesitamos implementar? Depende. <code>dlt</code> resume cada caso de uso con el siguiente diagrama:</p> <p></p> <p>Podemos cargar datos de diferentes maneras, dependiendo de los requisitos. Ver la documentaci\u00f3n para una introducci\u00f3n completa</p> <p>Podemos definir la disposici\u00f3n de escritura y la estrategia de escritura cuando ejecutamos un pipeline. Podemos elegir entre <code>\"replace\"</code>, <code>\"append\"</code>, <code>\"merge\"</code></p> <ol> <li><code>\"replace\"</code>: Reemplaza la tabla completamente</li> <li><code>\"append\"</code>: Carga filas incrementalmente, independientemente de sus valores</li> <li> <p><code>\"merge\"</code>: Inserta solo filas que son relevantes para la actualizaci\u00f3n, ej. que coinciden con datos en el objetivo seg\u00fan alguna estrategia y alguna clave que permite identificar filas coincidentes. Para la disposici\u00f3n \"merge\"</p> <ol> <li><code>\"delete-insert\"</code>: dado una coincidencia entre filas entrantes y existentes en alguna clave, DELETE la fila objetivo e INSERT desde las entrantes. Esta operaci\u00f3n es bloqueante o puede no ser soportada, dependiendo de la base de datos objetivo.</li> <li><code>\"upsert\"</code>: dado una coincidencia entre filas entrantes y existentes en alguna clave, UPDATE la fila objetivo e \"INSERT\" lo que ha cambiado de los datos entrantes. Esta operaci\u00f3n no es bloqueante pero puede no ser soportada, dependiendo de la base de datos objetivo.</li> <li><code>\"scd2\"</code>: dado una coincidencia entre filas entrantes y existentes en alguna clave, dejar la fila objetivo existente e \"INSERT\" una nueva de los datos entrantes. Usando algunas columnas adicionales, esto permite rastrear la validez del \u00faltimo valor, pero toma m\u00e1s espacio en disco.</li> </ol> </li> </ol>"},{"location":"4-switching-to-postgres/#configurando-una-base-de-datos-postgres","title":"Configurando una base de datos Postgres","text":"<p>Refiere a la secci\u00f3n Primeros Pasos para aprender c\u00f3mo configurar una base de datos postgres. Una vez que tengas una instancia postgres v\u00e1lida, necesitas configurar sus credenciales postgres.</p> <p>Algunos patrones de carga incremental no son soportados por <code>duckdb</code></p> <p>Duckdb es una base de datos anal\u00edtica, en memoria. Brilla para algunos tipos de tareas, y ofrece su propia implementaci\u00f3n de <code>MERGE</code>. Sin embargo, los patrones de carga incremental no son completamente soportados por <code>dlt</code> en este momento.</p>"},{"location":"4-switching-to-postgres/#configurar-dlt-para-usar-postgres","title":"Configurar <code>dlt</code> para usar Postgres","text":"<p>Necesitar\u00e1s decirle a <code>dlt</code> c\u00f3mo conectarse a tu base de datos Postgres. La forma recomendada es crear un archivo <code>secrets.toml</code> en la ra\u00edz de este proyecto con el siguiente contenido:</p> <pre><code>[sample_pipeline_postgres.destination.postgres.credentials]\nhost = \"localhost\"\nport = 5555\nuser = \"postgres\"\npassword = \"test\"\ndbname = \"postgres\"\n</code></pre> <p>Si deseas usar el m\u00e9todo de cadena de conexi\u00f3n, por ejemplo cuando uses una base de datos neon, tambi\u00e9n puedes hacerlo as\u00ed:</p> <pre><code>sample_pipeline_postgres.destination.postgres.credentials = 'postgresql://neondb_owner:&lt;password&gt;@&lt;host&gt;/neondb?sslmode=require&amp;channel_binding=require'\n</code></pre> <p>No hagas commit de tu archivo secrets.toml</p> <p>Aseg\u00farate de agregar <code>secrets.toml</code> a tu archivo <code>.gitignore</code> para evitar hacer commit de informaci\u00f3n sensible al control de versiones.</p> <p>Ajusta los par\u00e1metros de conexi\u00f3n seg\u00fan sea necesario</p> <p>Ajusta los par\u00e1metros de conexi\u00f3n (<code>host</code>, <code>port</code>, <code>user</code>, <code>password</code>, <code>dbname</code>) seg\u00fan tu configuraci\u00f3n de Postgres. Si est\u00e1s usando el devcontainer, el host deber\u00eda ser <code>postgres</code> y el puerto <code>5432</code>.</p> <p>Configura par\u00e1metros de conexi\u00f3n usando variables de entorno</p> <p>Tambi\u00e9n puedes configurar los par\u00e1metros de conexi\u00f3n usando variables de entorno. Ver la documentaci\u00f3n para m\u00e1s informaci\u00f3n.</p>"},{"location":"4-switching-to-postgres/#reemplazar-todo-usando-postgres","title":"Reemplazar todo usando postgres","text":"<p>Ahora puedes ejecutar el script <code>3_sample_pipeline_postgres_config.py</code> para probar la conexi\u00f3n y configuraci\u00f3n de Postgres.</p> <p>Las \u00fanicas cosas que han cambiado de los ejemplos anteriores son los par\u00e1metros <code>write_disposition</code> y <code>write_strategy</code> cuando se ejecuta el pipeline:</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    write_disposition={\n        \"disposition\": \"replace\",\n    },\n)\ndef sample_data() -&gt; Generator[dict, None, None]:\n    my_data = [\n        {\n            \"id\": 1,\n            \"name\": \"Mr. Mario\",\n            \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n            \"created_at\": \"2025-10-09 14:40:00\",\n            \"updated_at\": \"2025-10-09 14:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Mr. Luigi\",\n            \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n            \"created_at\": \"2025-10-08 16:15:00\",\n            \"updated_at\": \"2025-10-08 16:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n    ]\n    for item in my_data:\n        yield item\n</code></pre> <p>Si ejecutamos este ejemplo, deber\u00edamos ver salida similar a esta:</p> <pre><code>$ python dlt_tutorial/3_sample_pipeline_postgres_config.py\nStarting pipeline...\nPipeline run completed.\nPipeline sample_pipeline_postgres load step completed in 0.11 seconds\n1 load package(s) were loaded to destination postgres and into dataset sample_data\nThe postgres destination used postgresql://postgres:***@localhost:5555/postgres location to store data\nLoad package 1762003276.163356 is LOADED and contains no failed jobs\n</code></pre> <p>Ahora podemos conectarnos a nuestra base de datos Postgres y verificar el contenido de la tabla <code>sample_data.samples</code>:</p> <pre><code>$PGPASSWORD=test psql -h 0.0.0.0 -p 5555 -U postgres --pset expanded=auto -c \"select * from sample_data.samples;\"\n id |   name    |                 uuid                 |       created_at       |       updated_at       |     metadata__ingested_at     |        metadata__script_name         |   _dlt_load_id    |    _dlt_id     \n----+-----------+--------------------------------------+------------------------+------------------------+-------------------------------+--------------------------------------+-------------------+----------------\n  1 | Mr. Mario | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-09 14:50:00+00 | 2025-11-01 10:21:16.172598+00 | 3_sample_pipeline_postgres_config.py | 1762003276.163356 | MJaJ6AzyVleWlQ\n  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-01 10:21:16.172663+00 | 3_sample_pipeline_postgres_config.py | 1762003276.163356 | IrYyUJd1NAmnBQ\n(2 rows)\n</code></pre> \u00bfQu\u00e9 pasa si ejecutamos el script otra vez? <p>Si ejecutas el script otra vez, dado que el <code>write_disposition</code> est\u00e1 configurado como <code>\"replace\"</code> y el par\u00e1metro <code>refresh</code> est\u00e1 configurado como <code>\"drop_sources\"</code>, los datos existentes en la tabla <code>sample_data.samples</code> ser\u00e1n reemplazados con los nuevos datos obtenidos de la fuente, cada vez. Deber\u00edas ver diferentes marcas de tiempo <code>metadata__ingested_at</code>, y diferentes valores <code>_dlt_load_id</code> y <code>_dlt_id</code> con cada ejecuci\u00f3n.</p>"},{"location":"5-incremental-loading/","title":"Probando carga incremental","text":""},{"location":"5-incremental-loading/#una-pequena-desviacion-habilitando-refrescado-completo-a-traves-de-argumentos-de-linea-de-comandos","title":"Una peque\u00f1a desviaci\u00f3n: habilitando refrescado completo a trav\u00e9s de argumentos de l\u00ednea de comandos","text":"<p>Exploraremos estrategias de carga incremental en las siguientes secciones, pero primero, vamos a necesitar una manera f\u00e1cil de decirle a dlt c\u00f3mo manejar refrescados completos para que podamos comenzar r\u00e1pidamente desde cero, si algo sale mal.</p> <p>La estrategia de Refresh le dice a dlt si debe eliminar datos existentes en el destino antes de cargar nuevos datos. Las estrategias disponibles son, seg\u00fan la documentaci\u00f3n:</p> <ul> <li><code>drop_sources</code> - Eliminar tablas y estado de fuente y recurso para todas las fuentes actualmente siendo procesadas en m\u00e9todos run o extract del pipeline. (Nota: la historia del esquema es borrada)</li> <li><code>drop_resources</code>- Eliminar tablas y estado de recurso para todos los recursos siendo procesados. El estado a nivel de fuente no es modificado. (Nota: la historia del esquema es borrada)</li> <li><code>drop_data</code> - Limpiar todos los datos y estado de recurso para todos los recursos siendo procesados. El esquema no es modificado.</li> </ul> <pre><code>    pipeline = dlt.pipeline(\n        pipeline_name=\"sample_pipeline_postgres\",\n        destination=dlt.destinations.postgres,\n        dataset_name=\"sample_data\",\n    )\n\n    print(\"Starting pipeline...\")\n    load_info = pipeline.run(\n        sample_data,\n        table_name=\"samples\",\n        refresh=\"drop_sources\",\n    )\n</code></pre> <p>Para habilitar esta opci\u00f3n podemos modificar nuestro script de pipeline para incluir el par\u00e1metro <code>refresh</code> cuando creamos el pipeline.</p> <pre><code>def parse_args():\n    parser = argparse.ArgumentParser(description=\"Sample DLT Pipeline with Append\")\n    parser.add_argument(\n        \"--refresh\",\n        action=\"store_true\",\n        help=\"Refresh the data in the destination (if applicable)\",\n    )\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    should_refresh = args.refresh\n    pipeline = dlt.pipeline(\n        pipeline_name=\"sample_pipeline_postgres\",\n        destination=dlt.destinations.postgres,\n        dataset_name=\"sample_data\",\n    )\n    print(\"Running pipeline...\")\n    refresh_mode: TRefreshMode = \"drop_sources\"\n\n    if should_refresh:\n        print(\"Refreshing data in the destination.\")\n\n    load_info = pipeline.run(\n        sample_data,\n        table_name=\"samples\",\n        refresh=refresh_mode if should_refresh else None,\n    )\n</code></pre> <p>De esta manera, cuando ejecutemos nuestro pipeline con la bandera <code>--refresh</code>, eliminar\u00e1 datos existentes en el destino antes de cargar nuevos datos.</p> <p>Tambi\u00e9n implementamos el par\u00e1metro para simular cargar nuevos datos en las siguientes secciones. Modificamos nuestro <code>resource</code> basado en esta bandera.</p> <pre><code>def sample_data(use_new_data: bool = False) -&gt; Generator[dict, None, None]:\n    my_data = [\n        {\n            \"id\": 1,\n            \"name\": \"Mr. Mario\",\n            \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n            \"created_at\": \"2025-10-09 14:40:00\",\n            \"updated_at\": \"2025-10-09 14:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Mr. Luigi\",\n            \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n            \"created_at\": \"2025-10-08 16:15:00\",\n            \"updated_at\": \"2025-10-08 16:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n    ]\n\n    if use_new_data:\n        print(\"Using new data for this run.\")\n        my_data = [\n            {\n                \"id\": 1,\n                \"name\": \"Jumpman\",\n                \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n                \"created_at\": \"2025-10-09 14:40:00\",\n                \"updated_at\": \"2025-10-10 11:50:00\",\n                \"metadata\": {\n                    \"ingested_at\": dt.datetime.now().isoformat(),\n                    \"script_name\": Path(__file__).name,\n                },\n            },\n            {\n                \"id\": 3,\n                \"name\": \"Ms. Peach\",\n                \"uuid\": \"1a73f32f-9144-4318-9a00-4437bde41627\",\n                \"created_at\": \"2025-10-12 13:15:00\",\n                \"updated_at\": \"2025-10-13 13:50:00\",\n                \"metadata\": {\n                    \"ingested_at\": dt.datetime.now().isoformat(),\n                    \"script_name\": Path(__file__).name,\n                },\n            },\n        ]\n    for item in my_data:\n        yield item\n</code></pre> <p>Nuestra nueva interfaz de l\u00ednea de comandos se ve as\u00ed:</p> <pre><code>$ python dlt_tutorial/4_sample_pipeline_append.py --help\nusage: 4_sample_pipeline_append.py [-h] [--refresh]\n\nSample DLT Pipeline with Append\n\noptions:\n  -h, --help  show this help message and exit\n  --refresh   Refresh the data in the destination (if applicable)\n</code></pre> <p>y acepta un par\u00e1metro a trav\u00e9s del cual podemos simular cargar nuevos datos:</p> <pre><code>USE_NEW_DATA=1 python dlt_tutorial/4_sample_pipeline_append.py\n</code></pre>"},{"location":"5-incremental-loading/#solo-agregar","title":"Solo agregar","text":"<p>Ahora puedes ejecutar el pipeline con la bandera <code>--refresh</code> para comenzar desde cero:</p> <pre><code>$ python dlt_tutorial/4_sample_pipeline_append.py --refresh\nRunning pipeline...\nCustom parameter value: foo\nPipeline sample_pipeline_postgres load step completed in 0.09 seconds\n1 load package(s) were loaded to destination postgres and into dataset sample_data\nThe postgres destination used postgresql://postgres:***@localhost:5555/postgres location to store data\nLoad package 1762043727.4561055 is LOADED and contains no failed jobs\nDone\n</code></pre> <p>y ejecutarlo otra vez sin la bandera <code>--refresh</code> para agregar nuevos datos:</p> <pre><code>$ python dlt_tutorial/4_sample_pipeline_append.py --new-data\nRunning pipeline...\nCustom parameter value: foo\nPipeline sample_pipeline_postgres load step completed in 0.08 seconds\n1 load package(s) were loaded to destination postgres and into dataset sample_data\nThe postgres destination used postgresql://postgres:***@localhost:5555/postgres location to store data\nLoad package 1762044000.123456 is LOADED and contains no failed jobs\nDone\n</code></pre> <p>Puedes verificar el contenido de la tabla <code>sample_data</code> en Postgres para ver los resultados:</p> <pre><code>SELECT * FROM sample_data.samples;\n</code></pre> \u00bfQu\u00e9 pasa si ejecutamos el pipeline m\u00faltiples veces sin la bandera <code>--refresh</code>? <p>Cada vez que ejecutemos el pipeline, nuevos datos ser\u00e1n agregados a los datos existentes en el destino. Esto es porque estamos usando la estrategia de carga por defecto <code>append</code>, que agrega nuevos registros a la tabla existente sin modificar o eliminar registros existentes.</p> <pre><code> id |   name    |                 uuid                 |       created_at       |       updated_at       |     metadata__ingested_at     |    metadata__script_name    |    _dlt_load_id    |    _dlt_id     \n----+-----------+--------------------------------------+------------------------+------------------------+-------------------------------+-----------------------------+--------------------+----------------\n  1 | Mr. Mario | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-09 14:50:00+00 | 2025-11-02 18:30:16.036039+00 | 4_sample_pipeline_append.py | 1762119016.0306315 | pJA1hF4HneOUbw\n  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-02 18:30:16.036077+00 | 4_sample_pipeline_append.py | 1762119016.0306315 | UxcgfhkMgMleKg\n  1 | Mr. Mario | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-09 14:50:00+00 | 2025-11-02 18:30:20.517005+00 | 4_sample_pipeline_append.py | 1762119020.5093243 | HMDNsEtH+RBPXQ\n  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-02 18:30:20.517057+00 | 4_sample_pipeline_append.py | 1762119020.5093243 | SbekuWD7fosP1Q\n(4 rows)\n</code></pre>"},{"location":"5-incremental-loading/#agregar-con-clave-primaria-incremental","title":"Agregar con clave primaria incremental","text":"<p>En nuestro ejemplo anterior, pudimos agregar nuevos datos al destino, pero no ten\u00edamos una manera de identificar \u00fanicamente cada registro. Esto puede llevar a registros duplicados si los mismos datos son cargados m\u00faltiples veces. Una alternativa es usar una clave primaria incremental para identificar \u00fanicamente cada registro.</p> <p>Esto puede hacerse en dos pasos:</p> <ol> <li> <p>Modificar el decorador del recurso para especificar la columna <code>id</code> como clave primaria.</p> <pre><code>@dlt.resource(primary_key=\"id\", write_disposition=\"append\")\ndef sample_data(use_new_data: bool = False) -&gt; Generator[dict, None, None]:\n</code></pre> </li> <li> <p>Aplicar pistas al recurso para especificar que la columna <code>id</code> debe ser tratada como una clave primaria incremental. <code>dlt</code> permite esto usando el m\u00e9todo <code>apply_hints</code> en el recurso.</p> <pre><code>    # add unique and incremental primary key on \"id\" column\n    hinted_data = sample_data.apply_hints(incremental=dlt.sources.incremental(\"id\"))\n\n    load_info = pipeline.run(\n        hinted_data,\n        table_name=\"samples\",\n        refresh=refresh_mode if should_refresh else None,\n    )\n</code></pre> </li> </ol> <p>Ver m\u00e1s sobre pistas de esquema en la documentaci\u00f3n.</p> <p>la pista incremental solo filtra de datos entrantes</p> <p><code>dlt.sources.incremental</code> es recomendado cuando quieres reducir la cantidad de datos extra\u00eddos de tu fuente seleccionando solo datos nuevos o actualizados desde tu \u00faltima extracci\u00f3n de datos.</p> <p>Ahora puedes ejecutar el pipeline modificado con la bandera <code>--refresh</code> para comenzar desde cero:</p> <pre><code>$ python dlt_tutorial/4b_sample_pipeline_append_pk.py --refresh\n# salida omitida por brevedad\n</code></pre> <p>y ejecutarlo otra vez sin la bandera <code>--refresh</code> para agregar nuevos datos:</p> <pre><code>$ python dlt_tutorial/4b_sample_pipeline_append_pk.py\n# salida omitida por brevedad\n0 load package(s) were loaded to destination postgres and into dataset None\nThe postgres destination used postgresql://postgres:***@localhost:5555/postgres location to store data\nDone\n</code></pre> <p>Ver\u00e1s que no se agregaron nuevos registros a la tabla <code>sample_data</code>. Puedes verificar el contenido de la tabla <code>sample_data</code> en Postgres para ver los resultados:</p> <pre><code>SELECT * FROM sample_data.samples;\n</code></pre> \u00bfQu\u00e9 pasa si ejecutamos el pipeline m\u00faltiples veces sin la bandera <code>--refresh</code>? <p>Cada vez que ejecutemos el pipeline, nuevos datos ser\u00e1n agregados a los datos existentes en el destino. Sin embargo, dado que hemos especificado la columna <code>id</code> como clave primaria, <code>dlt</code> asegurar\u00e1 que no haya registros duplicados basados en esta clave. Si un registro con el mismo <code>id</code> ya existe en el destino, los datos entrantes con el mismo <code>id</code> ser\u00e1n descartados.</p> <p>Intenta ejecutar el pipeline ahora pasando la variable de entorno <code>USE_NEW_DATA=1</code> para simular cargar nuevos datos:</p> <pre><code>$ USE_NEW_DATA=1 python dlt_tutorial/4b_sample_pipeline_append_pk.py\n# salida omitida por brevedad\n</code></pre> \u00bfCu\u00e1l es el resultado de ejecutar el pipeline con <code>USE_NEW_DATA=1</code>? <p>Cuando ejecutas el pipeline con <code>USE_NEW_DATA=1</code>, la funci\u00f3n del recurso genera un nuevo conjunto de datos que incluye registros con valores <code>id</code> que ya existen en el destino. Sin embargo, dado que hemos especificado la columna <code>id</code> como clave primaria y aplicado la pista incremental, <code>dlt</code> descartar\u00e1 cualquier registro entrante que tenga un <code>id</code> que ya existe en el destino. Como resultado, solo nuevos registros con valores <code>id</code> \u00fanicos ser\u00e1n agregados al destino.</p> <pre><code> id |   name    |                 uuid                 |       created_at       |       updated_at       |     metadata__ingested_at     |      metadata__script_name      |    _dlt_load_id    |    _dlt_id     \n----+-----------+--------------------------------------+------------------------+------------------------+-------------------------------+---------------------------------+--------------------+----------------\n  1 | Mr. Mario | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-09 14:50:00+00 | 2025-11-02 15:23:52.179837+00 | 4b_sample_pipeline_append_pk.py | 1762107832.1758425 | 5kO1pi9oN0z/vQ\n  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-02 15:23:52.179866+00 | 4b_sample_pipeline_append_pk.py | 1762107832.1758425 | BpMmoBJH+6WA/A\n  3 | Ms. Peach | 1a73f32f-9144-4318-9a00-4437bde41627 | 2025-10-12 13:15:00+00 | 2025-10-13 13:50:00+00 | 2025-11-02 15:24:00.04396+00  | 4b_sample_pipeline_append_pk.py | 1762107840.0396352 | 4ZUmjQ7Pt9N/uQ\n(3 rows)\n</code></pre>"},{"location":"5-incremental-loading/#estrategias-de-fusion","title":"Estrategias de fusi\u00f3n","text":"<p><code>dlt</code> tambi\u00e9n soporta estrategias de <code>merge</code>, que te permiten actualizar registros existentes en el destino bas\u00e1ndose en una clave especificada.</p> <p>La disposici\u00f3n de escritura merge puede ser usada con tres estrategias diferentes:</p> <ol> <li><code>delete-insert</code>: La estrategia delete-insert carga datos a un conjunto de datos de staging, deduplica los datos de staging si se proporciona una primary_key, elimina los datos del destino usando merge_key y primary_key, y luego inserta los nuevos registros.</li> <li><code>upsert</code>: actualizar registro si la clave existe en la tabla objetivo, o insertar registro si la clave no existe en la tabla objetivo</li> <li><code>scd2</code>: Estrategia Slowly Changing Dimensions tipo 2, que rastrea cambios hist\u00f3ricos en datos creando nuevos registros para cada cambio mientras preserva registros anteriores. Agregar\u00e1 una nueva columna para rastrear el per\u00edodo de validez de cada registro.</li> </ol> <p>Para implementar estas estrategias, necesitamos modificar el par\u00e1metro <code>write_disposition</code> cuando creamos el pipeline.</p>"},{"location":"5-incremental-loading/#upsert","title":"Upsert","text":"<p>Podemos eliminar el m\u00e9todo <code>apply_hints</code> ya que no estamos usando una clave primaria incremental en este ejemplo.</p> <p>Si queremos usar la estrategia <code>upsert</code>, podemos ejecutar el pipeline modificado con la bandera <code>--refresh</code> para comenzar desde cero:</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition={\"disposition\": \"merge\", \"strategy\": \"upsert\"},\n)\ndef sample_data(use_new_data: bool = False) -&gt; Generator[dict, None, None]:\n</code></pre> <pre><code>$ python dlt_tutorial/5_sample_pipeline_merge_upsert.py --refresh\n# salida omitida por brevedad\n</code></pre> <p>y ejecutarlo otra vez sin la bandera <code>--refresh</code> para hacer upsert de nuevos datos:</p> <pre><code>$ python dlt_tutorial/5_sample_pipeline_merge_upsert.py\n# salida omitida por brevedad\n</code></pre> <p>Deber\u00edas ver que no se agregaron registros a la tabla <code>sample_data</code>. Puedes verificar el contenido de la tabla <code>sample_data</code> en Postgres para ver los resultados:</p> <pre><code>SELECT * FROM sample_data;\n</code></pre> <p>Intenta ahora ejecutar el pipeline pasando la variable de entorno <code>USE_NEW_DATA=1</code> para simular cargar nuevos datos:</p> <pre><code>$ USE_NEW_DATA=1 python dlt_tutorial/5_sample_pipeline_merge_upsert.py\n# salida omitida por brevedad\n</code></pre> \u00bfCu\u00e1l es el resultado de ejecutar el pipeline con <code>USE_NEW_DATA=1</code>? <p>Cuando ejecutas el pipeline con <code>USE_NEW_DATA=1</code>, la funci\u00f3n del recurso genera un nuevo conjunto de datos que incluye registros con valores <code>id</code> que ya existen en el destino. Dado que estamos usando la estrategia <code>upsert</code>, <code>dlt</code> actualizar\u00e1 registros existentes en el destino si un registro con el mismo <code>id</code> ya existe, o insertar\u00e1 nuevos registros si el <code>id</code> no existe.</p> <pre><code>postgres=# select * from sample_data.samples;\n id |   name    |                 uuid                 |       created_at       |       updated_at       |     metadata__ingested_at     |       metadata__script_name       |    _dlt_load_id    |_dlt_id\n----+-----------+--------------------------------------+------------------------+------------------------+-------------------------------+-----------------------------------+--------------------+----------------\n  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-02 18:37:17.901958+00 | 5_sample_pipeline_merge_upsert.py | 1762119437.8970616 | M6/KlLzJ2FeV/w\n  1 | Jumpman   | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-10 11:50:00+00 | 2025-11-02 18:37:23.170644+00 | 5_sample_pipeline_merge_upsert.py | 1762119443.1653655 | z5jIxXIbUnmbKw\n  3 | Ms. Peach | 1a73f32f-9144-4318-9a00-4437bde41627 | 2025-10-12 13:15:00+00 | 2025-10-13 13:50:00+00 | 2025-11-02 18:37:23.170657+00 | 5_sample_pipeline_merge_upsert.py | 1762119443.1653655 | hKdQ/VhGat+Pgw\n(3 rows)\n</code></pre>"},{"location":"5-incremental-loading/#slowly-changing-dimensions-scd2","title":"Slowly Changing Dimensions (SCD2)","text":"<p>De la misma manera, podemos implementar la estrategia <code>scd2</code> modificando el par\u00e1metro <code>write_disposition</code> cuando creamos el pipeline.</p> <p>Dado una coincidencia entre filas entrantes y existentes en alguna clave, dejar la fila objetivo existente e \"INSERT\" un nuevo registro de los datos entrantes, usando algunas columnas auxiliares.</p> <p>Esto permite rastrear la validez del \u00faltimo valor, pero toma m\u00e1s espacio en disco.</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition={\"disposition\": \"merge\", \"strategy\": \"scd2\"},\n)\ndef sample_data(use_new_data: bool = False) -&gt; Generator[dict, None, None]:\n</code></pre> <p>En la pr\u00e1ctica, <code>dlt</code> calcular\u00e1 una clave sustituta para cada registro basada en la clave primaria y el hash del contenido del registro.</p> <p>Cuando se encuentra un registro con la misma clave primaria pero contenido diferente, se inserta un nuevo registro con una nueva clave sustituta, mientras que el registro existente se marca como expirado.</p> <p>Puedes ejecutar el pipeline modificado con la bandera <code>--refresh</code> para comenzar desde cero:</p> <pre><code>$ python dlt_tutorial/6_sample_pipeline_merge_scd2.py --refresh\n# salida omitida por brevedad\n</code></pre> <p>y ejecutarlo otra vez sin la bandera <code>--refresh</code> para hacer upsert de nuevos datos:</p> <pre><code>$ python dlt_tutorial/6_sample_pipeline_merge_scd2.py\n# salida omitida por brevedad  \n</code></pre> <p>Dado que nuestros datos tienen una columna de metadatos llamada <code>metadata__ingested_at</code> que est\u00e1 basada en la marca de tiempo de ejecuci\u00f3n, <code>dlt</code> calcular\u00e1 una clave sustituta diferente cada vez que se inserte un registro.</p> <p>Esto efectivamente insertar\u00e1 nuevas filas cada vez que ejecutemos el pipeline, y marcar\u00e1 las filas anteriores como expiradas.</p> <pre><code>postgres=# select * from sample_data.samples;\n        _dlt_valid_from        |        _dlt_valid_to         | id |   name    |                 uuid                 |       created_at       |       updated_at       |     metadata__ingested_at     |      metadata__script_name      |    _dlt_load_id    |    _dlt_id     \n-------------------------------+------------------------------+----+-----------+--------------------------------------+------------------------+------------------------+-------------------------------+---------------------------------+--------------------+----------------\n 2025-11-02 21:43:43.942528+00 | 2025-11-02 21:46:07.88226+00 |  1 | Mr. Mario | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-09 14:50:00+00 | 2025-11-02 18:43:43.948416+00 | 6_sample_pipeline_merge_scd2.py | 1762119823.9425282 | 2PDbMZWckGbEzQ\n 2025-11-02 21:43:43.942528+00 | 2025-11-02 21:46:07.88226+00 |  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-02 18:43:43.94847+00  | 6_sample_pipeline_merge_scd2.py | 1762119823.9425282 | zFQAhPCh1tzs2A\n 2025-11-02 21:46:07.88226+00  |                              |  1 | Mr. Mario | a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc | 2025-10-09 14:40:00+00 | 2025-10-09 14:50:00+00 | 2025-11-02 18:46:07.887446+00 | 6_sample_pipeline_merge_scd2.py | 1762119967.8822596 | I6WPZYVDBhg9zQ\n 2025-11-02 21:46:07.88226+00  |                              |  2 | Mr. Luigi | 8c804ede-f8ae-409e-964d-9e355a3094e0 | 2025-10-08 16:15:00+00 | 2025-10-08 16:50:00+00 | 2025-11-02 18:46:07.887478+00 | 6_sample_pipeline_merge_scd2.py | 1762119967.8822596 | CrpVG8J0ezqiQg\n(4 rows)\n</code></pre> \u00bfC\u00f3mo sabemos cu\u00e1l es el valor m\u00e1s reciente cuando usamos SCD2? <p>Cuando usas la estrategia SCD2, cada registro tiene dos columnas adicionales: <code>_dlt_valid_from</code> y <code>_dlt_valid_to</code>. La columna <code>_dlt_valid_from</code> indica la marca de tiempo cuando el registro se volvi\u00f3 v\u00e1lido, mientras que la columna <code>_dlt_valid_to</code> indica la marca de tiempo cuando el registro fue reemplazado por una versi\u00f3n m\u00e1s nueva.</p> <p>Para encontrar el valor m\u00e1s reciente para una clave primaria dada, puedes consultar registros donde la columna <code>_dlt_valid_to</code> es <code>NULL</code>, ya que esto indica que el registro es actualmente v\u00e1lido.</p> <p>Por ejemplo, para encontrar los registros m\u00e1s recientes en la tabla <code>sample_data.samples</code>, puedes ejecutar la siguiente consulta SQL:</p> <pre><code>SELECT * FROM sample_data.samples WHERE _dlt_valid_to IS NULL;\n</code></pre> <p>Esto devolver\u00e1 solo los registros que son actualmente v\u00e1lidos, permiti\u00e9ndote ver los valores m\u00e1s recientes para cada clave primaria.</p> \u00bfQu\u00e9 pasa si ejecutas el pipeline usando la variable de entorno <code>USE_NEW_DATA=1</code>? <p>En los nuevos datos ver\u00e1s que <code>Mr. Luigi</code> no est\u00e1 presente. Esto significa que cuando ejecutes el pipeline con <code>USE_NEW_DATA=1</code>, este registro ser\u00e1 marcado como expirado en la tabla de destino (Un hard delete), y un nuevo registro para <code>Jumpman</code> ser\u00e1 insertado.</p>"},{"location":"5-incremental-loading/#resumiendo","title":"Resumiendo","text":"<p>Hemos explorado diferentes estrategias de carga incremental usando <code>dlt</code>, incluyendo <code>append</code>, <code>upsert</code> y <code>SCD2</code>. Cada estrategia tiene sus propios casos de uso y beneficios, dependiendo de los requisitos de tu pipeline de datos.</p> <p><code>dlt</code> ofrece m\u00e1s estrategias y opciones para carga incremental. Refiere a la documentaci\u00f3n de dlt para m\u00e1s informaci\u00f3n sobre c\u00f3mo implementar estas estrategias en tus pipelines de datos.</p>"},{"location":"6-schema-validation/","title":"Validaci\u00f3n de esquemas y Contratos de datos","text":"<p>Textualmente de los documentos</p> <ul> <li>El esquema describe la estructura de datos normalizados (ej., tablas, columnas, tipos de datos, etc.) y proporciona instrucciones sobre c\u00f3mo los datos deben ser procesados y cargados.</li> <li>dlt genera esquemas de los datos durante el proceso de normalizaci\u00f3n. Los usuarios pueden afectar este comportamiento est\u00e1ndar proporcionando pistas que cambian c\u00f3mo las tablas, columnas y otros metadatos son generados y c\u00f3mo los datos son cargados.</li> <li>Tales pistas pueden ser pasadas en el c\u00f3digo, ej., al decorador <code>dlt.resource</code> o m\u00e9todo <code>pipeline.run</code>. Los esquemas tambi\u00e9n pueden ser exportados e importados como archivos, que pueden ser modificados directamente.</li> <li><code>dlt</code> asocia un esquema con una fuente y un esquema de tabla con un recurso.</li> </ul> <p>En resumen, <code>dlt</code> inferir\u00e1 el esquema pero podemos forzarlo a tipos expl\u00edcitos</p> <ol> <li>Usando una especificaci\u00f3n <code>dict</code></li> <li>Usando un modelo <code>Pydantic</code></li> <li>Usando <code>hints</code></li> </ol>"},{"location":"6-schema-validation/#definiciones-de-esquema","title":"Definiciones de esquema","text":"<p>Los ejemplos usan la disposici\u00f3n de escritura <code>replace</code></p> <p>Los ejemplos en esta secci\u00f3n usan la disposici\u00f3n de escritura <code>replace</code> por simplicidad. En un escenario de producci\u00f3n, puedes querer usar <code>append</code> o <code>merge</code> para preservar datos existentes.</p> <p>Dado nuestro ejemplo anterior, podemos definir el esquema para el recurso <code>sample_data</code> usando una especificaci\u00f3n <code>dict</code>:</p> <pre><code>    my_data = [\n        {\n            \"id\": 1,\n            \"name\": \"Mr. Mario\",\n            \"uuid\": \"a6d7b6dd-bcdb-422e-83eb-f53b2eb4f2cc\",\n            \"created_at\": \"2025-10-09 14:40:00\",\n            \"updated_at\": \"2025-10-09 14:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Mr. Luigi\",\n            \"uuid\": \"8c804ede-f8ae-409e-964d-9e355a3094e0\",\n            \"created_at\": \"2025-10-08 16:15:00\",\n            \"updated_at\": \"2025-10-08 16:50:00\",\n            \"metadata\": {\n                \"ingested_at\": dt.datetime.now().isoformat(),\n                \"script_name\": Path(__file__).name,\n            },\n        },\n    ]\n</code></pre> <p>Usando <code>dlt</code> podemos definir el esquema de la siguiente manera:</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition=\"replace\",\n    columns={\n        \"id\": {\"data_type\": \"bigint\"},\n        \"name\": {\"data_type\": \"text\"},\n        \"uuid\": {\"data_type\": \"text\"},\n        \"created_at\": {\"data_type\": \"timestamp\"},\n        \"updated_at\": {\"data_type\": \"timestamp\"},\n    },\n    nested_hints={\n        \"metadata\": dlt.mark.make_nested_hints(\n            columns=[\n                {\n                    \"name\": \"ingested_at\",\n                    \"data_type\": \"timestamp\",\n                },\n                {\n                    \"name\": \"script_name\",\n                    \"data_type\": \"text\",\n                },\n            ]\n        ),\n    },\n    schema_contract={\n        \"tables\": \"evolve\",\n        \"columns\": \"freeze\",\n        \"data_type\": \"evolve\",\n    },\n)\ndef sample_data() -&gt; TDataItems:\n</code></pre> <p>Por ahora, ignora el argumento <code>schema_contract</code>, ya que lo explicaremos m\u00e1s tarde.</p> <p>Para campos anidados se vuelve un poco m\u00e1s complejo. Podemos usar el argumento <code>nested_columns</code> para definir el esquema para campos anidados:</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition=\"replace\",\n    columns={\n        \"id\": {\"data_type\": \"bigint\"},\n        \"name\": {\"data_type\": \"text\"},\n        \"uuid\": {\"data_type\": \"text\"},\n        \"created_at\": {\"data_type\": \"timestamp\"},\n        \"updated_at\": {\"data_type\": \"timestamp\"},\n    },\n    nested_hints={\n        \"metadata\": dlt.mark.make_nested_hints(\n            columns=[\n                {\n                    \"name\": \"ingested_at\",\n                    \"data_type\": \"timestamp\",\n                },\n                {\n                    \"name\": \"script_name\",\n                    \"data_type\": \"text\",\n                },\n            ]\n        ),\n    },\n    schema_contract={\n        \"tables\": \"evolve\",\n        \"columns\": \"freeze\",\n        \"data_type\": \"evolve\",\n    },\n)\ndef sample_data() -&gt; TDataItems:\n</code></pre>"},{"location":"6-schema-validation/#usando-modelos-pydantic","title":"Usando modelos Pydantic","text":"<p>Alternativamente, podemos usar modelos <code>Pydantic</code> para definir el esquema:</p> <pre><code>class SampleDataMetadataModel(BaseModel):\n    ingested_at: dt.datetime\n    script_name: str\n\n\nclass SampleDataModel(BaseModel):\n    id: int\n    name: str\n    uuid: UUID\n    created_at: dt.datetime\n    updated_at: dt.datetime\n    metadata: SampleDataMetadataModel\n</code></pre> <p>Y reemplazar el argumento <code>columns</code> en el decorador <code>dlt.resource</code> con el modelo <code>Pydantic</code>:</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition=\"replace\",\n    columns=SampleDataModel,\n    schema_contract={\n        \"tables\": \"evolve\",\n        \"columns\": \"freeze\",\n        \"data_type\": \"freeze\",\n    },\n)\ndef sample_data() -&gt; TDataItems:\n</code></pre>"},{"location":"6-schema-validation/#contratos-de-datos","title":"Contratos de datos","text":"<p>Anteriormente mencionamos el argumento <code>schema_contract</code> en el decorador <code>dlt.resource</code>. Esto nos permite definir un contrato de datos que especifica c\u00f3mo <code>dlt</code> debe manejar cambios de esquema durante la carga de datos.</p> <p><code>dlt</code> manejar\u00e1 cambios en tablas, columnas y tipos de datos por defecto. Puedes configurar su comportamiento expl\u00edcitamente pasando valores al argumento <code>schema_contract</code> del decorador <code>dlt.resource</code>, como:</p> <pre><code>@dlt.resource(\n schema_contract={\n        \"tables\": \"evolve\",\n        \"columns\": \"freeze\",\n        \"data_type\": \"freeze\",\n    })\ndef my_resource():\n  ...\n</code></pre> <p>Puedes controlar las siguientes entidades de esquema:</p> <ul> <li><code>tables</code> - el contrato se aplica cuando se crea una nueva tabla</li> <li><code>columns</code> - el contrato se aplica cuando se crea una nueva columna en una tabla existente</li> <li><code>data_type</code> - el contrato se aplica cuando los datos no pueden ser coercionados a un tipo de datos asociado con una columna existente.</li> </ul> <p>Puedes usar modos de contrato para decirle a <code>dlt</code> c\u00f3mo aplicar el contrato para una entidad particular:</p> <ul> <li><code>evolve</code>: Sin restricciones en cambios de esquema.</li> <li><code>freeze</code>: Esto lanzar\u00e1 una excepci\u00f3n si se encuentran datos que no se ajustan al esquema existente, por lo que no se cargar\u00e1n datos al destino.</li> <li><code>discard_row</code>: Esto descartar\u00e1 cualquier fila extra\u00edda si no se adhiere al esquema existente, y esta fila no ser\u00e1 cargada al destino.</li> <li><code>discard_value</code>: Esto descartar\u00e1 datos en una fila extra\u00edda que no se adhiere al esquema existente, y la fila ser\u00e1 cargada sin estos datos.</li> </ul> <p>\u00bfC\u00f3mo funciona \"evolve\"?</p> <p>El modo por defecto (evolve) funciona de la siguiente manera:</p> <ol> <li>Nuevas tablas siempre pueden ser creadas.</li> <li>Nuevas columnas siempre pueden ser agregadas a la tabla existente.</li> <li>Datos que no se coercionan al tipo de datos existente de una columna particular ser\u00e1n enviados a una columna variante creada para este tipo particular.</li> </ol>"},{"location":"6-schema-validation/#probando-cumplimiento-de-contratos","title":"Probando cumplimiento de contratos","text":"<p>Considera la primera definici\u00f3n de esquema usando una especificaci\u00f3n <code>dict</code>, donde definimos que ni las columnas ni los tipos de datos deben cambiar:</p> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition=\"replace\",\n    columns={\n        \"id\": {\"data_type\": \"bigint\"},\n        \"name\": {\"data_type\": \"text\"},\n        \"uuid\": {\"data_type\": \"text\"},\n        \"created_at\": {\"data_type\": \"timestamp\"},\n        \"updated_at\": {\"data_type\": \"timestamp\"},\n    },\n    nested_hints={\n        \"metadata\": dlt.mark.make_nested_hints(\n            columns=[\n                {\n                    \"name\": \"ingested_at\",\n                    \"data_type\": \"timestamp\",\n                },\n                {\n                    \"name\": \"script_name\",\n                    \"data_type\": \"text\",\n                },\n            ]\n        ),\n    },\n    schema_contract={\n        \"tables\": \"evolve\",\n        \"columns\": \"freeze\",\n        \"data_type\": \"evolve\",\n    },\n)\ndef sample_data() -&gt; TDataItems:\n</code></pre> <pre><code>@dlt.resource(\n    name=\"sample_data\",\n    primary_key=\"id\",\n    write_disposition=\"replace\",\n    columns=SampleDataModel,\n    schema_contract={\n        \"tables\": \"evolve\",\n        \"columns\": \"freeze\",\n        \"data_type\": \"freeze\",\n    },\n)\ndef sample_data() -&gt; TDataItems:\n</code></pre> \u00bfQu\u00e9 pasa si configuramos <code>id=1</code> a <code>id='oops, esto es un string ahora'</code> en su lugar? <p>Deber\u00edas obtener un error como este:</p> <pre><code>&lt;class 'dlt.normalize.exceptions.NormalizeJobFailed'&gt;\nJob for `job_id=samples.e278217ef2.typed-jsonl.gz` failed terminally in load with `load_id=1762124329.823492` with message: In schema `sample_pipeline_postgres`: In Table: `samples` Column: `id__v_text` . Contract on `data_type` with `contract_mode=freeze` is violated. Can't add variant column `id__v_text` for table `samples` because `data_types` are frozen. Offending data item: id: None.\n</code></pre> <p>Si intentamos lo mismo con la definici\u00f3n de esquema del modelo Pydantic, obtendremos un error similar:</p> <p>Revisa la documentaci\u00f3n para m\u00e1s detalles</p> <p>Ver https://dlthub.com/docs/general-usage/schema-contracts para m\u00e1s informaci\u00f3n sobre contratos de datos y validaci\u00f3n de esquemas.</p>"},{"location":"7-next-steps/","title":"Conclusiones y Pr\u00f3ximos Pasos","text":"<p>Esperamos que a estas alturas tengas una buena comprensi\u00f3n de c\u00f3mo funciona DLT y c\u00f3mo usarlo para construir pipelines de datos. Aunque la documentaci\u00f3n de dlt es bastante extensa, a\u00fan est\u00e1 un poco \u00e1spera en los bordes y puede ser dif\u00edcil de navegar a veces. Tratamos de cubrir lo que siento son los aspectos m\u00e1s importantes de dlt para empezar, pero hay mucho m\u00e1s por explorar.</p> <p>Aqu\u00ed hay algunas sugerencias para qu\u00e9 hacer a continuaci\u00f3n:</p>"},{"location":"7-next-steps/#pregunta-a-la-comunidad-y-el-dlthub-bot-en-slack","title":"Pregunta a la comunidad y el <code>dlthub bot</code> en Slack","text":"<p>Si tienes preguntas o necesitas ayuda, puedes unirte a la comunidad DLT Slack y hacer tus preguntas all\u00ed. La comunidad es muy activa y \u00fatil.</p>"},{"location":"7-next-steps/#explora-la-cli-de-dlt","title":"Explora la CLI de <code>dlt</code>","text":"<p>La interfaz de l\u00ednea de comandos (CLI) de <code>dlt</code> proporciona algunos comandos para gestionar y ejecutar tus pipelines. Puedes explorar los comandos disponibles ejecutando:</p> <pre><code>dlt --help\n</code></pre> <p>Puede ser de particular inter\u00e9s explorar los comandos de dashboard:</p> <pre><code>dlt pipeline sample_pipeline show\n</code></pre> <p>Refiere a la documentaci\u00f3n oficial para m\u00e1s detalles sobre los comandos disponibles y su uso.</p>"},{"location":"7-next-steps/#usa-el-comando-dlt-init-y-beneficiate-de-plantillas-de-proyecto","title":"Usa el comando <code>dlt init</code> y benef\u00edciate de plantillas de proyecto","text":"<p>Aunque en la documentaci\u00f3n oficial esto se introduce muy temprano, lo encuentro m\u00e1s \u00fatil una vez que tienes una mejor comprensi\u00f3n de c\u00f3mo funciona dlt. El comando <code>dlt init</code> te permite crear un nuevo proyecto dlt desde una plantilla. Puedes explorar las plantillas disponibles ejecutando:</p> <pre><code>dlt init &lt;SOURCE&gt; &lt;DESTINATION&gt;\n</code></pre> <p>Por ejemplo, prueba:</p> <pre><code>mkdir my_dlt_project\ncd my_dlt_project\ndlt init postgres duckdb\n</code></pre> <p>Esto iniciar\u00e1 un nuevo proyecto dlt que extrae datos de una base de datos Postgres y los carga en una base de datos DuckDB (Y algunas otras cosas tambi\u00e9n).</p> <p>Las plantillas pueden ser abrumadoras al principio</p> <p>El c\u00f3digo generado puede ser abrumador al principio, pero esperamos que ahora entiendas c\u00f3mo las diferentes partes funcionan juntas.</p> <p>Puedes verificar las plantillas disponibles usando:</p> <pre><code>dlt init --list-sources\n</code></pre> <p>y</p> <pre><code>dlt init --list-destinations\n</code></pre>"},{"location":"7-next-steps/#explora-tutoriales-y-cursos-mas-avanzados","title":"Explora tutoriales y cursos m\u00e1s avanzados","text":"<p>Si sientes que <code>dlt</code> es una buena opci\u00f3n para tus necesidades de carga de datos, puedes explorar tutoriales y cursos m\u00e1s avanzados disponibles en el sitio de documentaci\u00f3n oficial en https://dlthub.com/docs/tutorial/education:</p> <ul> <li>https://dlthub.com/docs/tutorial/fundamentals-course</li> <li>https://dlthub.com/docs/tutorial/advanced-course</li> </ul> <p>\u00a1Hasta la pr\u00f3xima, feliz carga de datos!</p>"}]}